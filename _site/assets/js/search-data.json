{"0": {
    "doc": "Home",
    "title": "Key Dates and Deadlines",
    "content": ". | Workshop dates: 3rd - 7th January, 2023; in person at W Hotel, Abu Dhabi | Submission portal opens: October 1st, 2022 | Paper submission deadline: November 6th, 2022 | Acceptance and travel grant notification: November 20th, 2022 | . The top accepted papers will be recommended for inclusion in a future special issue of the IEEE Journal of Selected Topics in Signal Processing. ",
    "url": "/#key-dates-and-deadlines",
    "relUrl": "/#key-dates-and-deadlines"
  },"1": {
    "doc": "Home",
    "title": "Workshop Themes",
    "content": "In the past decade, deep learning has demonstrated unprecedented performance across many different domains. Modern neural networks learn high-performing nonlinear representations for complex multi-modal data (e.g. text and image) without labels; they generate near-photorealistic high-resolution images from random noise; and they efficiently reconstruct data from compressive, corrupted measurements in various imaging and structural prediction tasks. However, despite recent endeavors, the underlying principles behind their success remain shrouded in mystery. At a fundamental level, each of these successes, and numerous others in scientific and engineering applications, stem from the low-dimensional structure present both in the training data and in the networks themselves. This presents a significant opportunity to bring insights from better-understood low-dimensional models to the setting of deep learning, where models are notoriously plagued by issues of poor resource and data efficiency, robustness, and generalization. Given these exciting but relatively less-exploited connections, this workshop aims to bring together experts in machine learning, applied mathematics, signal processing, and optimization, to share recent progress and foster collaborations on the mathematical foundations of deep learning. As the community continues to embrace the power of deep learning, with unprecedented new challenges in terms of modeling and interpretability, we hope to stimulate vibrant discussions towards bridging the gap between the theory and practice of deep learning, with low-dimensionality as a unifying focus. ",
    "url": "/#workshop-themes",
    "relUrl": "/#workshop-themes"
  },"2": {
    "doc": "Home",
    "title": "Confirmed Speakers",
    "content": "Information on the speakers’ planned talks is available here. <!-- ",
    "url": "/#confirmed-speakers",
    "relUrl": "/#confirmed-speakers"
  },"3": {
    "doc": "Home",
    "title": "Confirmed Distinguished Speakers",
    "content": "--> Misha Belkin . UC San Diego . Beidi Chen . Meta . Ivan Dokmanić . University of Basel . Simon Du . University of Washington . Reinhard Heckel . TU Munich . Gitta Kutyniok . LMU Munich . Jason Lee . Princeton . Qi Lei . NYU . Yi Ma . UC Berkeley . Rina Panigrahy . Google . Tomaso Poggio . MIT . Qing Qu . UMich . Ohad Shamir . Weizmann Institute . Mahdi Soltanolkotabi . USC . Daniel Soudry . Technion . Weijie Su . UPenn . René Vidal . Johns Hopkins . Yu-Xiang Wang . UCSB . Bihan Wen . NTU . Fanny Yang . ETH Zurich . ",
    "url": "/",
    "relUrl": "/"
  },"4": {
    "doc": "Home",
    "title": "Workshop Schedule",
    "content": "All times in GST (GMT+4). ",
    "url": "/#workshop-schedule",
    "relUrl": "/#workshop-schedule"
  },"5": {
    "doc": "Home",
    "title": "Morning sessions 9 AM - 12 PM; Afternoon sessions 1:30 PM - 6 PM",
    "content": "Tuesday, Jan 3 (Morning) Educational tutorials (Chair: Qing Qu) 9:00 AM - 10:00 AM John Wright (Columbia; remote) 10:00 AM - 11:00 AM Yuqian Zhang (Rutgers) 11:00 AM - 11:15 AM Coffee break 11:15 AM - 12:15 PM Sam Buchanan (TTIC) Tuesday, Jan 3 (Afternoon) Educational tutorials (Chair: Sam Buchanan) 1:30 PM - 2:30 PM Zhihui Zhu (Ohio State; remote) 2:30 PM - 3:30 PM Yi Ma (UC Berkeley) 3:30 PM - 4:00 PM Coffee break 4:00 PM - 5:00 PM Atlas Wang (UT Austin) 5:00 PM - 6:00 PM Saiprasad Ravishankar (Michigan State) Wednesday, Jan 4 (Morning) Invited Talks: Theory + Optimization (Chair: Qing Qu) 8:30 AM - 9:00 AM Opening (Yi Ma) 9:00 AM - 10:00 AM Tomaso Poggio (MIT) 10:00 AM - 11:00 AM Simon Du (UW) 11:00 AM - 11:15 AM Coffee break and poster session 11:15 AM - 12:15 PM Qi Lei (NYU) Wednesday, Jan 4 (Afternoon) Invited Talks: Image Recovery (Chair: Saiprasad Ravishankar) 1:30 PM - 2:30 PM René Vidal (JHU) 2:30 PM - 3:30 PM Mahdi Soltanolkotabi (USC) 3:30 PM - 4:00 PM Coffee break and poster session 4:00 PM - 5:00 PM Reinhard Heckel (TU Munich) 5:00 PM - 6:00 PM Bihan Wen (NTU) 6:00 PM - 6:30 PM Poster session Thursday, Jan 5 (Morning) Invited Talks: Representation Learning (Chair: Yi Ma) 8:30 AM - 9:00 AM Arrival 9:00 AM - 10:00 AM Yi Ma (UC Berkeley) 10:00 AM - 11:00 AM Weijie Su (UPenn) 11:00 AM - 11:15 AM Coffee break and poster session 11:15 AM - 12:15 PM Qing Qu (UMich) Thursday, Jan 5 (Afternoon) Invited Talks: Implicit Bias (Chair: Yuqian Zhang) 1:30 PM - 2:30 PM Misha Belkin (UCSD) 2:30 PM - 3:30 PM Ohad Shamir (Weizmann Institute) 3:30 PM - 4:00 PM Coffee break and poster session 4:00 PM - 5:00 PM Yu-Xiang Wang (UCSB) 5:00 PM - 6:00 PM Panel, moderated by Jeremias Sulam (Yi Ma, Jason Lee, Eric Xing) 6:00 PM - 6:30 PM Poster session Friday, Jan 6 (Morning) Invited Talks: Deep Learning + Systems (Chair: Atlas Wang) 8:30 AM - 9:00 AM Arrival 9:00 AM - 10:00 AM Gitta Kutyniok (LMU Munich) 10:00 AM - 11:00 AM Beidi Chen (Meta; remote) 11:00 AM - 11:15 AM Coffee break and poster session 11:15 AM - 12:15 AM Rina Panigrahy (Google) Friday, Jan 6 (Afternoon) Invited Talks: Data + Architectures (Chair: Sam Buchanan) 1:30 PM - 2:30 PM Daniel Soudry (Technion) 2:30 PM - 3:30 PM Ivan Dokmanić (Univ. of Basel) 3:30 PM - 4:00 PM Coffee break and poster session 4:00 PM - 5:00 PM Jason Lee (Princeton) 5:00 PM - 6:00 PM Fanny Yang (ETH Zurich) 6:00 PM - 6:30 PM Poster session Saturday, Jan 7 Social Event: Tour of Le Louvre Abu Dhabi 9:45 AM - 12:30 PM Tour departs from the W Hotel ",
    "url": "/#morning-sessions-9-am-12-pm-afternoon-sessions-1-30-pm-6-pm",
    "relUrl": "/#morning-sessions-9-am-12-pm-afternoon-sessions-1-30-pm-6-pm"
  },"6": {
    "doc": "Home",
    "title": "Organizers",
    "content": "Yi Ma . UC Berkeley . General Chair . Eric P. Xing . MBZUAI . General Chair (Local) . Samuel Horváth . MBZUAI . Local Chair . Karthik Nandakumar . MBZUAI . Local Chair . Martin Takáč . MBZUAI . Local Chair . Qing Qu . University of Michigan . Program Chair and Publication Chair . Jeremias Sulam . Johns Hopkins University . Program Chair . Atlas Wang . UT Austin . Program Chair . John Wright . Columbia University . Program Chair . Yuqian Zhang . Rutgers University . Program Chair . Yuejie Chi . Carnegie Mellon University . Publicity Chair . Chong You . Google NYC . Publication Chair . Zhihui Zhu . Ohio State University . Publication Chair . Sam Buchanan . TTIC . Tutorial Chair . Saiprasad Ravishankar . Michigan State University . Online Chair . ",
    "url": "/#organizers",
    "relUrl": "/#organizers"
  },"7": {
    "doc": "Home",
    "title": "Host Institution",
    "content": " ",
    "url": "/",
    "relUrl": "/"
  },"8": {
    "doc": "Home",
    "title": "Home",
    "content": "Third Workshop on Seeking Low&#8209;Dimensionality in Deep&nbsp;Neural&nbsp;Networks January 2023,&nbsp;MBZUAI Join Us in Abu Dhabi from 3-7 January 2023! . Live Stream | Workshop Schedule . Tuesday, January 3rd: Tutorials in Studio 5 at the W Hotel . Wednesday, January 4rd – Friday, January 6th: Talks in Great Hall 1 at the W Hotel . Announcing the workshop venue: W Abu Dhabi Hotel at Yas Island . Register here for in-person and remote participation . ",
    "url": "/",
    "relUrl": "/"
  },"9": {
    "doc": "Announcements",
    "title": "Announcements",
    "content": "Announcements are stored in the _announcements directory and rendered according to the layout file, _layouts/announcement.html. ",
    "url": "/announcements/",
    "relUrl": "/announcements/"
  },"10": {
    "doc": "Announcements",
    "title": "Week 1 Announcement",
    "content": "Apr 8 &middot; 0 min read . | Create a new repository based on Just the Class. | Configure a publishing source for GitHub Pages. Your course website is now live! | Update _config.yml with your course information. | Edit and create .md Markdown files to add your content. | . ",
    "url": "/announcements/",
    "relUrl": "/announcements/"
  },"11": {
    "doc": "Announcements",
    "title": "Week 0 Announcement",
    "content": "Apr 1 &middot; 0 min read Hello world! . ",
    "url": "/announcements/",
    "relUrl": "/announcements/"
  },"12": {
    "doc": "Calendar",
    "title": "Calendar",
    "content": ". | 8:00 AM | 8:30 AM | 9:00 AM | 9:30 AM | 10:00 AM | 10:30 AM | 11:00 AM | 11:30 AM | 12:00 PM | 12:30 PM | 1:00 PM | 1:30 PM | 2:00 PM | 2:30 PM | 3:00 PM | 3:30 PM | 4:00 PM | 4:30 PM | 5:00 PM | 5:30 PM | . | ",
    "url": "/calendar/",
    "relUrl": "/calendar/"
  },"13": {
    "doc": "Calendar",
    "title": "Tuesday, Jan 3",
    "content": ". | Opening Ceremony 8:30 AM–9:00 AM | asdf 11:30 AM–12:30 PM 310 Soda | Office Hours 12:30 PM–2:00 PM 271 Soda | . | ",
    "url": "/calendar/",
    "relUrl": "/calendar/"
  },"14": {
    "doc": "Calendar",
    "title": "Wednesday",
    "content": ". | Lecture 9:30 AM–10:30 AM 150 Wheeler | Section 11:30 AM–12:30 PM 310 Soda | Office Hours 12:30 PM–2:00 PM 271 Soda | . | ",
    "url": "/calendar/",
    "relUrl": "/calendar/"
  },"15": {
    "doc": "Calendar",
    "title": "Thursday",
    "content": "| ",
    "url": "/calendar/",
    "relUrl": "/calendar/"
  },"16": {
    "doc": "Calendar",
    "title": "Friday",
    "content": ". | Lecture 9:30 AM–10:30 AM 150 Wheeler | Section 11:30 AM–12:30 PM 310 Soda | Office Hours 12:30 PM–2:00 PM 271 Soda | . | ",
    "url": "/calendar/",
    "relUrl": "/calendar/"
  },"17": {
    "doc": "Calendar",
    "title": "Saturday, Jan 7",
    "content": "| . ",
    "url": "/calendar/",
    "relUrl": "/calendar/"
  },"18": {
    "doc": "Organizers",
    "title": "Organizers",
    "content": "Yi Ma . UC Berkeley . General Chair . Eric P. Xing . MBZUAI . General Chair (Local) . Samuel Horváth . MBZUAI . Local Chair . Karthik Nandakumar . MBZUAI . Local Chair . Martin Takáč . MBZUAI . Local Chair . Qing Qu . University of Michigan . Program Chair and Publication Chair . Jeremias Sulam . Johns Hopkins University . Program Chair . Atlas Wang . UT Austin . Program Chair . John Wright . Columbia University . Program Chair . Yuqian Zhang . Rutgers University . Program Chair . Yuejie Chi . Carnegie Mellon University . Publicity Chair . Chong You . Google NYC . Publication Chair . Zhihui Zhu . Ohio State University . Publication Chair . Sam Buchanan . TTIC . Tutorial Chair . Saiprasad Ravishankar . Michigan State University . Online Chair . ",
    "url": "/organizers/#organizers",
    "relUrl": "/organizers/#organizers"
  },"19": {
    "doc": "Organizers",
    "title": "Organizers",
    "content": "Third Workshop on Seeking Low&#8209;Dimensionality in Deep&nbsp;Neural&nbsp;Networks January 2023,&nbsp;MBZUAI ",
    "url": "/organizers/",
    "relUrl": "/organizers/"
  },"20": {
    "doc": "Past workshops",
    "title": "Past workshops",
    "content": ". | 2021 | 2020 | . ",
    "url": "/past_workshops/#past-workshops",
    "relUrl": "/past_workshops/#past-workshops"
  },"21": {
    "doc": "Past workshops",
    "title": "Past workshops",
    "content": "Third Workshop on Seeking Low&#8209;Dimensionality in Deep&nbsp;Neural&nbsp;Networks January 2023,&nbsp;MBZUAI ",
    "url": "/past_workshops/",
    "relUrl": "/past_workshops/"
  },"22": {
    "doc": "Accepted Papers",
    "title": "Poster Presentations",
    "content": "Authors of accepted papers will present their work at one of three poster sessions at the workshop venue. A list of the accepted papers and the poster session assignments is given below, along with logistics information about the poster sessions. ",
    "url": "/posters/#poster-presentations",
    "relUrl": "/posters/#poster-presentations"
  },"23": {
    "doc": "Accepted Papers",
    "title": "Poster Session Logistics",
    "content": "There are three evening poster sessions, on Wednesday, Thursday, and Friday – see the schedule. The poster sessions will be held in the entryway at the W Hotel – follow signage for the SLowDNN workshop. On the day of your poster session, you will be able to hang your poster up for the entire day. In addition, please note: . | The space available per poster is 40 inches wide by 50 inches high, so please ensure your poster will fit. (We recommend a 36 x 24 inch size.) | Your poster may be in any visual format you wish. | Please print and bring your poster with you as you travel. Unfortunately, MBZUAI does not have any official in-house facility for printing posters. | . ",
    "url": "/posters/#poster-session-logistics",
    "relUrl": "/posters/#poster-session-logistics"
  },"24": {
    "doc": "Accepted Papers",
    "title": "Accepted Papers and Poster Session Assignments",
    "content": " ",
    "url": "/posters/#accepted-papers-and-poster-session-assignments",
    "relUrl": "/posters/#accepted-papers-and-poster-session-assignments"
  },"25": {
    "doc": "Accepted Papers",
    "title": "Wednesday, January 4th",
    "content": "TT-NF: Tensor Train Neural Fields Anton Obukhov, Mikhail Usvyatsov, Christos Sakaridis, Konrad Schindler, Luc Van Gool Robust Calibration with Multi-domain Temperature Scaling Yaodong Yu, Stephen Bates, Yi Ma, Michael Jordan On the infinite-depth limit of finite-width neural networks Soufiane Hayou Combining Deep Learning and Adaptive Sparse Modeling for Low-dose CT Reconstruction Ling Chen, Zhishen Huang, Yong Long, Saiprasad Ravishankar Lottery Pools: Winning More by Interpolating Tickets without Increasing Training or Inference Cost Lu Yin, Shiwei Liu, Meng Fang, Tianjin Huang, Vlado Menkovski, Mykola Pechenizkiy Sparsity May Cry: Let Us Fail (Current) Sparse Neural Networks Together! Shiwei Liu, Tianlong Chen, Zhenyu Zhang, Xuxi Chen, Tianjin Huang, AJAY KUMAR JAISWAL, Zhangyang Wang Deep Unfolded Tensor Robust PCA with Self-supervised Learning Harry Dong, Megna Shah, Sean Donegan, Yuejie Chi Closed-form Solutions of Learning Dynamics for Two-layer Nets for Collapsed Orthogonal Data Yutong Wang, Qing Qu, Wei Hu Closed-Loop Transcription via Convolutional Sparse Coding Xili Dai, Ke Chen, Shengbang Tong, Jingyuan Zhang, Xingjian Gao, Mingyang Li, Druv Pai, Yuexiang Zhai, Xiaojun Yuan, Heung-Yeung Shum, Lionel Ni, Yi Ma On the Ability of Graph Neural Networks to Model Interactions Between Vertices Noam Razin, Tom Verbin, Nadav Cohen On the Geometry of Reinforcement Learning in Continuous State and Action Spaces Saket Tiwari, Omer Gottesman, George Konidaris State-driven Implicit Modeling for Sparsity and Robustness in Neural Networks Alicia Y. Tsai, Juliette Decugis, Laurent El Ghaoui, Alper Atamturk Robust Self-Guided Deep Image Prior Shijun Liang, Evan Bell, Saiprasad Ravishankar, Qing Qu Sparse-view Cone Beam CT Reconstruction using Data-consistent Supervised and Adversarial Learning from Scarce Training Data Gabriel Maliakal, Anish Lahiri, Marc Louis Klasky, Jeffrey A Fessler, Saiprasad Ravishankar Robustness of sparse local Lipschitz predictors Ramchandran Muthukumar, Jeremias Sulam Reverse Engineering $\\ell_p$ attacks: A block-sparse optimization approach with recovery guarantees Darshan Thaker, Paris Giampouras, Rene Vidal ",
    "url": "/posters/#wednesday-january-4th",
    "relUrl": "/posters/#wednesday-january-4th"
  },"26": {
    "doc": "Accepted Papers",
    "title": "Thursday, January 5th",
    "content": "From Optimization Dynamics to Generalization Bounds via Lojasiewicz Gradient Inequality Fusheng Liu, Haizhao Yang, Soufiane Hayou, Qianxiao Li Unsupervised Manifold Linearizing and Clustering Tianjiao Ding, Shengbang Tong, Kwan Ho Ryan Chan, Xili Dai, Yi Ma, Benjamin David Haeffele Pursuit of a Discriminative Representation for Multiple Subspaces via Sequential Games Druv Pai, Michael Psenka, Chih-Yuan Chiu, Manxi Wu, Edgar Dobriban, Yi Ma VQ-Flows: Vector Quantized Local Normalizing Flows Chris Barton Dock, Sahil Sidheekh, Maneesh Kumar Singh, Radu Balan Latent-space disentanglement with untrained generator networks allows to isolate different motion types in video data Abdullah, Martin Holler, Malena Sabate Landman, Karl Kunisch Robust Training under Label Noise by Over-parameterization Sheng Liu, Zhihui Zhu, Qing Qu, Chong You Linear Convergence Analysis of Neural Collapse with Unconstrained Features Peng Wang, Huikang Liu, Can Yaras, Laura Balzano, Qing Qu Deep Learning meets Nonparametric Regression: Are Weight-Decayed DNNs Locally Adaptive? Kaiqi Zhang, Yu-Xiang Wang Sparse MoE with Random Routing as the New Dropout: Training Bigger and Self-Scalable Models Tianlong Chen, Zhenyu Zhang, AJAY KUMAR JAISWAL, Shiwei Liu, Zhangyang Wang Finding Better Descent Directions for Adversarial Training Fabian Latorre, Igor Krawczuk, Leello Tadesse Dadi, Thomas Pethick, Volkan Cevher APP: Anytime Progressive Pruning Diganta Misra, Bharat Runwal, Tianlong Chen, Zhangyang Wang, Irina Rish Effects of Data Geometry in Early Deep Learning Saket Tiwari, George Konidaris Dimension Mixer Model: Group Mixing of Input Dimensions for Efficient Function Approximation Suman Sapkota, Binod Bhattarai PSPS: Preconditioned Stochastic Polyak Step-size method for badly scaled data Farshed Abdukhakimov, XIANG CHULU, Dmitry Kamzolov, Robert M. Gower, Martin Takac Lifted Bregman Training of Neural Networks Xiaoyu Wang, Martin Benning DynamicViT: Making Vision Transformer faster through layer skipping Amanuel Negash Mersha, Sammy Assefa Robustness via deep low rank representations Amartya Sanyal, Puneet K. Dokania, Varun Kanade, Philip Torr ",
    "url": "/posters/#thursday-january-5th",
    "relUrl": "/posters/#thursday-january-5th"
  },"27": {
    "doc": "Accepted Papers",
    "title": "Friday, January 6th",
    "content": "Neural Collapse with Normalized Features: A Geometric Analysis over the Riemannian Manifold Can Yaras, Peng Wang, Zhihui Zhu, Laura Balzano, Qing Qu Intrinsic dimensionality and generalization properties of the $\\mathcal{R}$-norm inductive bias Clayton Sanford, Navid Ardeshir, Daniel Hsu SinkGAT: Doubly-Stochastic Graph Attention Tianlin Liu, Cheng Shi, Anastasis Kratsios, Ivan Dokmanic SMUG: Towards Robust MRI Reconstruction by Smoothed Unrolling Hui Li, Jinghan Jia, Shijun Liang, Yuguang Yao, Saiprasad Ravishankar, Sijia Liu Semi-private learning via low dimensional structures Yaxi Hu, Francesco Pinto, Amartya Sanyal, Fanny Yang Certified Defenses Against Near-Subspace Unrestricted Adversarial Attacks Ambar Pal, Rene Vidal Representation Learning Through Manifold Flattening and Reconstruction Michael Psenka, Druv Pai, Vishal G Raman, Shankar Sastry, Yi Ma Flat minima generalize for low-rank matrix recovery Lijun Ding, Dmitriy Drusvyatskiy, Maryam Fazel Are All Losses Created Equal: A Neural Collapse Perspective Jinxin Zhou, Chong You, Xiao Li, Kangning Liu, Sheng Liu, Qing Qu, Zhihui Zhu Fast Evaluation of Multilinear Operations in Convolutional Tensorial Neural Networks Tahseen Rabbani, Jiahao Su, Xiaoyu Liu, David Chan, Geoffrey Sangston, Furong Huang Dimensionality compression and expansion in Deep Neural Networks Stefano Recanatesi, Matthew Farrell, Madhu Advani, Timothy Moore, Guillaume Lajoie, Eric Todd SheaBrown A picture of the space of typical learnable tasks Rahul Ramesh, Jialin Mao, Itay Griniasty, Rubing Yang, Han Kheng Teoh, Mark Transtrum, James Sethna, Pratik Chaudhari Deep Reinforcement Learning based Unrolling Network for MRI Reconstruction Chong Wang, Rongkai Zhang, Gabriel Maliakal, Saiprasad Ravishankar, Bihan Wen Bilevel learning of $\\ell_{1}$ regularizers with closed-form gradients (BLORC) Avrajit Ghosh, Saiprasad Ravishankar Resource-Efficient Invariant Networks: Exponential Gains by Unrolled Optimization Sam Buchanan, Jingkai Yan, Ellie Haber, John Wright ",
    "url": "/posters/#friday-january-6th",
    "relUrl": "/posters/#friday-january-6th"
  },"28": {
    "doc": "Accepted Papers",
    "title": "Accepted Papers",
    "content": "Third Workshop on Seeking Low&#8209;Dimensionality in Deep&nbsp;Neural&nbsp;Networks January 2023,&nbsp;MBZUAI ",
    "url": "/posters/",
    "relUrl": "/posters/"
  },"29": {
    "doc": "Schedule",
    "title": "Workshop Schedule",
    "content": "All times in GST (GMT+4). ",
    "url": "/schedule/#workshop-schedule",
    "relUrl": "/schedule/#workshop-schedule"
  },"30": {
    "doc": "Schedule",
    "title": "Morning sessions 9 AM - 12 PM; Afternoon sessions 1:30 PM - 6 PM",
    "content": "Tuesday, Jan 3 (Morning) Educational tutorials (Chair: Qing Qu) 9:00 AM - 10:00 AM John Wright (Columbia; remote) 10:00 AM - 11:00 AM Yuqian Zhang (Rutgers) 11:00 AM - 11:15 AM Coffee break 11:15 AM - 12:15 PM Sam Buchanan (TTIC) Tuesday, Jan 3 (Afternoon) Educational tutorials (Chair: Sam Buchanan) 1:30 PM - 2:30 PM Zhihui Zhu (Ohio State; remote) 2:30 PM - 3:30 PM Yi Ma (UC Berkeley) 3:30 PM - 4:00 PM Coffee break 4:00 PM - 5:00 PM Atlas Wang (UT Austin) 5:00 PM - 6:00 PM Saiprasad Ravishankar (Michigan State) Wednesday, Jan 4 (Morning) Invited Talks: Theory + Optimization (Chair: Qing Qu) 8:30 AM - 9:00 AM Opening (Yi Ma) 9:00 AM - 10:00 AM Tomaso Poggio (MIT) 10:00 AM - 11:00 AM Simon Du (UW) 11:00 AM - 11:15 AM Coffee break and poster session 11:15 AM - 12:15 PM Qi Lei (NYU) Wednesday, Jan 4 (Afternoon) Invited Talks: Image Recovery (Chair: Saiprasad Ravishankar) 1:30 PM - 2:30 PM René Vidal (JHU) 2:30 PM - 3:30 PM Mahdi Soltanolkotabi (USC) 3:30 PM - 4:00 PM Coffee break and poster session 4:00 PM - 5:00 PM Reinhard Heckel (TU Munich) 5:00 PM - 6:00 PM Bihan Wen (NTU) 6:00 PM - 6:30 PM Poster session Thursday, Jan 5 (Morning) Invited Talks: Representation Learning (Chair: Yi Ma) 8:30 AM - 9:00 AM Arrival 9:00 AM - 10:00 AM Yi Ma (UC Berkeley) 10:00 AM - 11:00 AM Weijie Su (UPenn) 11:00 AM - 11:15 AM Coffee break and poster session 11:15 AM - 12:15 PM Qing Qu (UMich) Thursday, Jan 5 (Afternoon) Invited Talks: Implicit Bias (Chair: Yuqian Zhang) 1:30 PM - 2:30 PM Misha Belkin (UCSD) 2:30 PM - 3:30 PM Ohad Shamir (Weizmann Institute) 3:30 PM - 4:00 PM Coffee break and poster session 4:00 PM - 5:00 PM Yu-Xiang Wang (UCSB) 5:00 PM - 6:00 PM Panel, moderated by Jeremias Sulam (Yi Ma, Jason Lee, Eric Xing) 6:00 PM - 6:30 PM Poster session Friday, Jan 6 (Morning) Invited Talks: Deep Learning + Systems (Chair: Atlas Wang) 8:30 AM - 9:00 AM Arrival 9:00 AM - 10:00 AM Gitta Kutyniok (LMU Munich) 10:00 AM - 11:00 AM Beidi Chen (Meta; remote) 11:00 AM - 11:15 AM Coffee break and poster session 11:15 AM - 12:15 AM Rina Panigrahy (Google) Friday, Jan 6 (Afternoon) Invited Talks: Data + Architectures (Chair: Sam Buchanan) 1:30 PM - 2:30 PM Daniel Soudry (Technion) 2:30 PM - 3:30 PM Ivan Dokmanić (Univ. of Basel) 3:30 PM - 4:00 PM Coffee break and poster session 4:00 PM - 5:00 PM Jason Lee (Princeton) 5:00 PM - 6:00 PM Fanny Yang (ETH Zurich) 6:00 PM - 6:30 PM Poster session Saturday, Jan 7 Social Event: Tour of Le Louvre Abu Dhabi 9:45 AM - 12:30 PM Tour departs from the W Hotel ",
    "url": "/schedule/#morning-sessions-9-am-12-pm-afternoon-sessions-1-30-pm-6-pm",
    "relUrl": "/schedule/#morning-sessions-9-am-12-pm-afternoon-sessions-1-30-pm-6-pm"
  },"31": {
    "doc": "Schedule",
    "title": "Schedule",
    "content": "Third Workshop on Seeking Low&#8209;Dimensionality in Deep&nbsp;Neural&nbsp;Networks January 2023,&nbsp;MBZUAI ",
    "url": "/schedule/",
    "relUrl": "/schedule/"
  },"32": {
    "doc": "Speakers",
    "title": "Confirmed Speakers",
    "content": "Clicking a speaker’s photo will jump to their talk information below. <!-- ",
    "url": "/speakers/#confirmed-speakers",
    "relUrl": "/speakers/#confirmed-speakers"
  },"33": {
    "doc": "Speakers",
    "title": "Confirmed Distinguished Speakers",
    "content": "--> Misha Belkin . UC San Diego . Beidi Chen . Meta . Ivan Dokmanić . University of Basel . Simon Du . University of Washington . Reinhard Heckel . TU Munich . Gitta Kutyniok . LMU Munich . Jason Lee . Princeton . Qi Lei . NYU . Yi Ma . UC Berkeley . Rina Panigrahy . Google . Tomaso Poggio . MIT . Qing Qu . UMich . Ohad Shamir . Weizmann Institute . Mahdi Soltanolkotabi . USC . Daniel Soudry . Technion . Weijie Su . UPenn . René Vidal . Johns Hopkins . Yu-Xiang Wang . UCSB . Bihan Wen . NTU . Fanny Yang . ETH Zurich . ",
    "url": "/speakers/",
    "relUrl": "/speakers/"
  },"34": {
    "doc": "Speakers",
    "title": "Talk Details",
    "content": "Misha Belkin . UC San Diego . Title: Why do neural models need so many parameters? . Abstract . One of the striking aspects of modern neural networks is their extreme size reaching billions or even trillions parameters. Why are so many parameters needed? To attempt an answer to this question, I will discuss an algorithm and distribution independent non-asymptotic trade-off between the model size, excess test loss, and training loss for linear predictors and feature maps. Specifically, we show that models that perform well on the test data (have low excess loss) are either “classical” – have training loss close to the noise level, or are “modern” – have a much larger number of parameters compared to the minimum needed to fit the training data exactly. Furthermore, I will provide empirical evidence that optimal performance of realistic models is typically achieved in the “modern” regime, when they are trained below the noise level. Beidi Chen . Meta . Title: Hardware-aware Sparsity: Accurate and Efficient Foundation Model Training . Abstract . Foundation models trained on complex and rapidly growing data consume enormous computational resources. In this talk, I will describe our recent work on exploiting model and activation sparsity to accelerate foundation model training in both data and model parallel settings. We show that adapting algorithms on current hardware leads to efficient model training with no drop in accuracy. I will start by describing Pixelated Butterfly and Monarch, simple yet efficient sparse model training frameworks on GPUs. They use simple static block-sparse patterns based on butterfly and low-rank matrices, taking into account GPU block-oriented efficiency. They train up to 2.5x faster (wall-clock) than the dense Vision Transformer and GPT-2 counterparts with no drop in accuracy. Next, I will present AC-SGD, communication-efficient pipeline parallelism training frameworks over slow networks. Based on an interesting observation that model weights change slowly during the training, AC-SGD compresses activations or the change of activations with guarantees. It trains or fine-tunes DeBERTa and GPT2-1.5B 4.3x faster in slower networks without sacrificing model quality. I will conclude by outlining three future research directions - data efficiency,software-hardware codesign, and ML for science, and several ongoing projects including linear-time algorithm for large optimal transport problems, efficient autoregressive model (gpt3-style) inference, and ML for new material discovery. Ivan Dokmanić . University of Basel . Title: Statistical Mechanics of Graph Neural Networks . Abstract . Graph convolution networks are excellent models for relational data but their success is not well understood. I will show how ideas from statistical physics and random matrix theory allow us to precisely characterize GCN generalization on the contextual stochastic block model—a community-structured graph model with features. The resulting curves are rich: they predict double descent thus far unseen in graph learning and explain the qualitative distinction between learning on homophilic graphs (such as friendship networks) and heterophilic graphs (such as protein interaction networks). Earlier approaches based on VC-dimension or Rademacher complexity are too blunt to yield similar mechanistic insight. Our findings pleasingly translate to real “production-scale” networks and datasets and suggest simple redesigns which improve performance of state-of-the-art networks on heterophilic datasets. They further suggest intriguing connections with spectral graph theory, signal processing, and iterative methods for the Helmholtz equation. Joint work with Cheng Shi, Liming Pan, and Hong Hu. Simon Du . University of Washington . Title: Passive and Active Multi-Task Representation Learning . Abstract . Representation learning has been widely used in many applications. In this talk, I will present our work which uncovers when and why representation learning provably improves the sample efficiency, from a statistical learning point of view. Furthermore, I will talk about how to actively select the most relevant task to boost the performance. Reinhard Heckel . TU Munich . Title: The role of data and models for deep-learning based image reconstruction . Abstract . Deep-learning methods give state-of-the-art performance for a variety of imaging tasks, including accelerated magnetic resonance imaging. In this talk we discuss whether improved models and algorithms or training data are the most promising way forward. First, we ask whether increasing the model size and the training data improves performance in a similar fashion as it has in domains such as language modeling. We find that scaling beyond relatively few examples yields only marginal performance gains. Second, we discuss the robustness of deep learning based image reconstruction methods. Perhaps surprisingly, we find no evidence for neural networks being any less robust than classical reconstruction methods (such as l1 minimization). However, we find that both classical and deep learning based approaches perform significantly worse under distribution shifts, i.e., when trained (or tuned) and tested on slightly different data. Finally, we show that the out-of-distribution performance can be improved through more diverse training data, or through an algorithmic intervention called test-time-training. Gitta Kutyniok . LMU Munich . Title: The Next Generation of Reliable AI: From Digital to Analog Hardware . Abstract . Artificial intelligence is currently leading to one breakthrough after the other, in the sciences, in industry, and in public life. However, one current major drawback is the lack of reliability of such methodologies, which, in particular, also concerns almost any application of deep neural networks. In this lecture, we will first provide a short introduction into the world of reliability of deep neural networks, with one main focus being on explainability. We will, in particular, present a novel approach based on information theory, coined ShearletX, which allows to not only provide higher level explanations, but also reveals the reason for wrong decisions. We will then delve deeper and discuss fundamental limitations of numerous current deep learning-based approaches, showing that there do exist severe problems in terms of computability on any type of digital hardware, which seriously affects their reliability. But theory also shows a way out, pointing towards a future on analog hardware such as neuromorphic computing or quantum computing to achieve true reliability. Jason Lee . Princeton . Title: Feature Learning with gradient descent . Abstract . Significant theoretical work has established that in specific regimes, neural networks trained by gradient descent behave like kernel methods. However, in practice, it is known that neural networks strongly outperform their associated kernels. In this work, we explain this gap by demonstrating that there is a large class of functions which cannot be efficiently learned by kernel methods but can be easily learned with gradient descent on a two layer neural network outside the kernel regime by learning representations that are relevant to the target task. We also demonstrate that these representations allow for efficient transfer learning, which is impossible in the kernel regime. Specifically, we consider the problem of learning polynomials which depend on only a few relevant directions, i.e. of the form f⋆(x)=g(Ux) where U: \\R^d \\to \\R^r with d≫r. When the degree of f⋆ is p, it is known that n≍d^p samples are necessary to learn f⋆ in the kernel regime. Our primary result is that gradient descent learns a representation of the data which depends only on the directions relevant to f⋆. This results in an improved sample complexity of n≍rd^2+pd^r. Furthermore, in a transfer learning setup where the data distributions in the source and target domain share the same representation U but have different polynomial heads we show that a popular heuristic for transfer learning has a target sample complexity independent of d. This is joint work with Alex Damian and Mahdi Soltanolkotabi. Qi Lei . NYU . Title: Reconstructing Training Data from Model Gradient, Provably . Abstract . Understanding when and how much a model gradient leaks information about the training sample is an important question in privacy. In this talk, we present a surprising result: even without training or memorizing the data, we can fully reconstruct the training samples from a single gradient query at a randomly chosen parameter value. We prove the identifiability of the training data under mild conditions: with shallow or deep neural networks and a wide range of activation functions. We also present a statistically and computationally efficient algorithm based on low-rank tensor decomposition to reconstruct the training data. As a provable attack that reveals sensitive training data, our findings suggest potential severe threats to privacy, especially in federated learning. Yi Ma . UC Berkeley . Title: On the Principles of Parsimony and Self-Consistency – Structured Compressive Closed-Loop Transcription . Abstract . Ten years into the revival of deep networks and artificial intelligence, we propose a theoretical framework that sheds light on understanding deep networks within a bigger picture of intelligence in general. We introduce two fundamental principles, Parsimony and Self-consistency, that address two fundamental questions regarding Intelligence – what to learn and how to learn, respectively. We argue that these two principles can be realized in entirely measurable and computable ways for an important family of structures and models, known as a linear discriminative representation (LDR). The two principles naturally lead to an effective and efficient computational framework, known as a compressive closed-loop transcription, that unifies and explains the evolution of modern deep networks and modern practices of artificial intelligence. Within this framework, we will see how fundamental ideas in information theory, control theory, game theory, sparse coding, and optimization are closely integrated in such a closed-loop system, all as necessary ingredients to learn autonomously and correctly. We demonstrate the power of this framework for learning discriminative, generative, and autoencoding models for large-scale real-world visual data, with entirely white-box deep networks, under all settings (supervised, incremental, and unsupervised). We believe that these two principles are the cornerstones for the emergence of intelligence, artificial or natural, and the compressive closed-loop transcription is a universal learning engine that serves as the basic learning units for all autonomous intelligent systems, including the brain. Rina Panigrahy . Google . Title: How to learn a Table of Concepts? . Abstract . An idealized view of an intelligent system is one where there is a table of concepts from simple to complex and on each raw input (image or text) is processed to identify the concepts present or triggered by that input. While today’s ML systems may use a Mixture of Experts or Table of entities/mentions that are trying to map concepts to experts/entities/mention, there is hardly a clear picture of how concepts build upon each other automatically. In the domain of images, we can think of commonly occurring shapes, types of motions as concepts and ask how would a visual system create entries for different shape types in a table of concepts. How does one convert raw data into concepts given by something like their latent representation. A trivial transformation like random projection doesn’t work. But we will see how simple networks similar to ones used in practice can be viewed as “sketching operators” that can be used to create representations for images with simple shapes that are isomorphic to their polynomial representations. By putting such polynomial representations in a locality sensitive table, we obtain a dictionary of curves. And then by recursively applying another layer of the sketching operator on the curves one gets a dictionary of concepts such as shapes. Tomaso Poggio . MIT . Title: A key principle underlying deep networks: compositional sparsity . Abstract . A key question is whether there exist a theoretical explanation — a common motif — to the various network architectures, including the human brain, that perform so well in learning tasks. I will discuss the conjecture that this is compositional sparsity of effectively computable functions: all functions of many variables must effectively be compositionally sparse that is with constituent functions each depending on a small number of variables. Qing Qu . UMich . Title: Understanding Deep Neural Networks via Neural Collapse . Abstract . Recently, an intriguing phenomenon in the final stages of network training has been discovered and caught great interest, in which the last-layer features and classifiers collapse to simple but elegant mathematical structures: all training inputs are mapped to class-specific points in feature space, and the last-layer classifier converges to the dual of the features’ class means while attaining the maximum possible margin. This phenomenon, dubbed Neural Collapse, persists across a variety of different network architectures, datasets, and even data domains. Moreover, a progressive neural collapse occurs from shallow to deep layers. This talk leverages the symmetry and geometry of Neural Collapse, and develops a rigorous mathematical theory to explain when and why it happens under the so-called unconstrained feature model. Based upon this, we show how it can be used to provide guidelines to understand and improve transferability with more efficient fine-tuning. Ohad Shamir . Weizmann Institute . Title: Implicit bias in machine learning . Abstract . Most practical algorithms for supervised machine learning boil down to optimizing the average performance over a training dataset. However, it is increasingly recognized that although the optimization objective is the same, the manner in which it is optimized plays a decisive role in the properties of the resulting predictor. For example, when training large neural networks, there are generally many weight combinations that will perfectly fit the training data. However, gradient-based training methods somehow tend to reach those which, for example, do not overfit; are brittle to adversarially crafted examples; or have other unusual properties. In this talk, I’ll describe several recent theoretical and empirical results related to this question. Mahdi Soltanolkotabi . USC . Title: Demystifying Feature learning via gradient descent with applications to medical image reconstruction . Abstract . In this talk I will discuss the challenges and opportunities for using deep learning in medical image reconstruction. Contemporary techniques in this field rely on convolutional architectures that are limited by the spatial invariance of their filters and have difficulty modeling long-range dependencies. To remedy this, I will discuss our work on designing new transformer-based architectures called HUMUS-Net that lead to state of the art performance and do not suffer from these limitations. A key component in the success of the above approach is a unique feature learning capability of unrolled neural networks trained based on end-to-end training. In the second part of the talk I will demystify this feature learning capability of neural networks in this context as well as more broadly for other problems. Our result is based on an intriguing spectral bias phenomena for gradient descent, that puts the iterations on a particular trajectory towards solutions that learn good features that generalize well. Notably this analysis overcomes a major theoretical bottleneck in the existing literature and goes beyond the “lazy” training regime which requires unrealistic hyperparameter choices (e.g. very small step sizes, large initialization or wide models). Daniel Soudry . Technion . Title: How catastrophic can catastrophic forgetting be in linear regression? . Abstract . This is a two-part talk. 1) Deep neural nets typically forget old tasks when trained on new tasks. This phenomenon, called “catastrophic forgetting” is not well understood, even in the most basic setting of linear regression. Therefore, we study catastrophic forgetting when fitting an overparameterized linear model to a sequence of tasks with different input distributions. We analyze how much the model forgets the true labels of earlier tasks after training on subsequent tasks, obtaining exact expressions and bounds. In particular, when T tasks in d dimensions are presented cyclically for k iterations, we prove an upper bound of T^2 * min{1/sqrt(k), d/k} on the forgetting. This stands in contrast to the convergence to the offline solution, which can be arbitrarily slow according to existing alternating projection results. We further show that the T^2 factor can be lifted when tasks are presented in a random ordering. 2) We prove that the tendency of SGD to converge to dynamically stable (“flat”) minima leads to a step-size dependent bound on the smoothness of the learned function, in shallow ReLU nets. This implies that, although these networks are universal approximators, stable shallow networks are not. Namely, there is a function (that can be realized as a stable two hidden-layer ReLU network) but that cannot be approximated by stable single hidden-layer ReLU networks trained with a non-vanishing step size. Moreover, we prove that if a function is sufficiently smooth (Sobolev) then it can be approximated arbitrarily well using single hidden-layer ReLU networks that correspond to stable solutions of gradient descent. Weijie Su . UPenn . Title: Geometrization of Real-World Deep Neural Networks . Abstract . In this talk, we will investigate the emergence of geometric patterns in well-trained deep learning models by making use of a layer-peeled model and the law of equi-separation. The former is a nonconvex optimization program that models the last-layer features and weights. We use the model to shed light on the neural collapse phenomenon of Papyan, Han, and Donoho, and to predict a hitherto-unknown phenomenon that we term minority collapse in imbalanced training. This is based on joint work with Cong Fang, Hangfeng He, and Qi Long (arXiv:2101.12699). The law of equi-separation is a pervasive empirical phenomenon that describes how data are separated according to their class membership from the bottom to the top layer in a well-trained neural network. We will show that, through extensive computational experiments, neural networks improve data separation through layers in a simple exponential manner. This law leads to roughly equal ratios of separation that a single layer is able to improve, thereby showing that all layers are created equal. We will conclude the talk by discussing the implications of this law on the interpretation, robustness, and generalization of deep learning, as well as on the inadequacy of some existing approaches toward demystifying deep learning. This is based on joint work with Hangfeng He (arXiv:2210.17020). René Vidal . Johns Hopkins . Title: Principled Defenses and Reverse Engineering of Adversarial Attacks in a Union of Subspaces . Abstract . Deep neural network-based classifiers have been shown to be vulnerable to imperceptible perturbations to their input, such as ℓp-bounded norm adversarial attacks. This has motivated the development of many defense methods, which are then broken by new attacks, and so on. Recent work has also focused on the problem of reverse engineering adversarial attacks, which requires both recovering the clean signal and determining the type of attack (ℓ1, ℓ2 or ℓ∞). However, existing methods either do not come with provable guarantees, or they can certify the accuracy of the classifier only for very small perturbations. In this work, we assume that the data lies approximately in a union of low-dimensional linear subspaces and exploit this low-dimensional structure to develop a theory of adversarial robustness for subspace-sparse classifiers. We first derive geometric conditions on the subspaces under which any attacked signal can be decomposed as the sum of a clean signal plus an attack. We then derive norm-independent certification regions, showing that we can provably defend against specific unrestricted adversarial attacks. Yu-Xiang Wang . UCSB . Title: Deep Learning meets Nonparametric Regression: Are Weight-decayed DNNs locally adaptive? . Abstract . They say deep learning is just curve fitting. But how good is it in curve fitting exactly? Are DNNs as good as, or even better than, classical curve-fitting tools, e.g., splines and wavelets? In this talk, I will cover my group’s recent paper on this topic and some interesting insight. Specifically, I will provide new answers to “Why is DNN stronger than kernels?” “Why are deep NNs stronger than shallow ones?”, “Why ReLU?”, “How do DNNs generalize under overparameterization?”, “What is the role of sparsity in deep learning?”, “Is lottery ticket hypothesis real?” All in one package. Intrigued? Come to my talk and find out! . Bihan Wen . NTU . Title: Beyond Classic Image Priors: Deep Reinforcement Learning and Disentangling for Image Restoration . Abstract . The key to solving various ill-posed inverse problems, including many computational imaging and image restoration tasks, is to exploit effective image priors. The classic signal processing theory laid the foundation on constructing analytical image models such as sparse coding and low-rank approximation. Recent advances in machine learning, especially deep learning technologies have made incredible progress in the past few years, which enables people to rethink how the image prior can be formulated more effective. Despite those many deep learning methods achieved the state-of-the-art results in image restoration tasks, there are still limitations in practice, such as adversarial robustness, customization, generalization, and data-efficiency. In this talk, I will share some of our recent works on deep disentangling and deep reinforcement learning in image restoration tasks. We argue that learning more than just the classic image priors are needed for solving inverse problems to alleviate some of the limitations. We show promising results in several image restoration tasks, including denoising, adversarial purification, low-light image enhancement and computational imaging. Fanny Yang . ETH Zurich . Title: Strong inductive biases provably prevent harmless interpolation . Abstract . Interpolating models have recently gained popularity in the statistical learning community due to common practices in modern machine learning: complex models achieve good generalization performance despite interpolating high-dimensional training data. In this talk, we prove generalization bounds for high-dimensional linear models that interpolate noisy data generated by a sparse ground truth. In particular, we first show that minimum-l1-norm interpolators achieve high-dimensional asymptotic consistency at a logarithmic rate. Further, as opposed to the regularized or noiseless case, for min-lp-norm interpolators with 1&lt;p&lt;2 we surprisingly obtain polynomial rates. Our results suggest a new trade-off for interpolating models: a stronger inductive bias encourages a simpler structure better aligned with the ground truth at the cost of an increased variance. We finally discuss our latest results where we show that this phenomenon also holds for nonlinear models. ",
    "url": "/speakers/#talk-details",
    "relUrl": "/speakers/#talk-details"
  },"35": {
    "doc": "Speakers",
    "title": "Speakers",
    "content": "Third Workshop on Seeking Low&#8209;Dimensionality in Deep&nbsp;Neural&nbsp;Networks January 2023,&nbsp;MBZUAI ",
    "url": "/speakers/",
    "relUrl": "/speakers/"
  },"36": {
    "doc": "Host Institution",
    "title": "Host Institution",
    "content": " ",
    "url": "/sponsors/",
    "relUrl": "/sponsors/"
  },"37": {
    "doc": "Host Institution",
    "title": "Host Institution",
    "content": "Third Workshop on Seeking Low&#8209;Dimensionality in Deep&nbsp;Neural&nbsp;Networks January 2023,&nbsp;MBZUAI ",
    "url": "/sponsors/",
    "relUrl": "/sponsors/"
  },"38": {
    "doc": "Call for Papers",
    "title": "Key Dates",
    "content": ". | Workshop dates: 3rd - 7th January, 2023; in person at W Hotel, Abu Dhabi | Submission portal opens: October 1st, 2022 | Paper submission deadline: November 6th, 2022 | Acceptance and travel grant notification: November 20th, 2022 | . ",
    "url": "/submission/#key-dates",
    "relUrl": "/submission/#key-dates"
  },"39": {
    "doc": "Call for Papers",
    "title": "How to Submit Your Work",
    "content": "We will use OpenReview to manage submissions. Submit your work here: . https://openreview.net/group?id=mbzuai.ac.ae/SLowDNN/2023/Workshop . See the submission guidelines and topics of interest. ",
    "url": "/submission/#how-to-submit-your-work",
    "relUrl": "/submission/#how-to-submit-your-work"
  },"40": {
    "doc": "Call for Papers",
    "title": "Logistics for Accepted Papers",
    "content": "Accepted works will be expected to present a poster describing the work in-person at the workshop. Travel grants are available to support authors of accepted papers: see the travel page for details. A small subset of the top accepted papers will be recommended for inclusion in a future special issue of the IEEE Journal of Selected Topics in Signal Processing. ",
    "url": "/submission/#logistics-for-accepted-papers",
    "relUrl": "/submission/#logistics-for-accepted-papers"
  },"41": {
    "doc": "Call for Papers",
    "title": "Topics of Interest",
    "content": "Topics of interest include, but are not limited to, connections between low-dimensional models and the theory, architectures, algorithms, and applications of deep neural networks: . | Theory: approximation, generalization, robustness, compact and structured representations | Optimization: Benign non-convex optimization, implicit bias analysis, convergence guarantees | Architectures: compact/model-based/neuro-inspired/invariant neural networks | Algorithms: pruning, sparse training, robust training | Applications: generative models, resource/data-efficient learning, inverse problems | . ",
    "url": "/submission/#topics-of-interest",
    "relUrl": "/submission/#topics-of-interest"
  },"42": {
    "doc": "Call for Papers",
    "title": "Submission Guidelines",
    "content": "We aim to showcase: . | The latest research innovations at all stages of the research process, from work-in-progress to recently published papers | Position or survey papers on any topics relevant to this workshop (see the call above) | . Concretely, we ask members of the community to submit a conference-style paper (from four to eight pages, with extra pages for references) describing the work. Please also upload a short (250 word) abstract to OpenReview. Do not anonymize submissions. Papers should be written using the NeurIPS 2022 style files, available here. OpenReview submissions may also include any of the following supplemental materials that describe the work in further detail. | A poster (in PDF form) presenting results of work-in-progress. | A link to a blog post (e.g., distill.pub, Medium) describing results. | Appendices with detailed derivations and additional experiments. | . This workshop is non-archival, and it will not have proceedings. We permit under-review or concurrent submissions. Reviewing will be performed in a single-blind fashion (authors should not anonymize their submissions). ",
    "url": "/submission/#submission-guidelines",
    "relUrl": "/submission/#submission-guidelines"
  },"43": {
    "doc": "Call for Papers",
    "title": "Call for Papers",
    "content": "Third Workshop on Seeking Low&#8209;Dimensionality in Deep&nbsp;Neural&nbsp;Networks January 2023,&nbsp;MBZUAI ",
    "url": "/submission/",
    "relUrl": "/submission/"
  },"44": {
    "doc": "Travel",
    "title": "Travel Support",
    "content": "MBZUAI has travel grants in the amount of up to $1000 USD available for up to forty authors of accepted papers. All accepted papers will be considered for a travel grant. Accepted papers are expected to be presented in-person. To submit work, see the submission page. Travel grants will be announced concurrently with paper acceptances. ",
    "url": "/travel/#travel-support",
    "relUrl": "/travel/#travel-support"
  },"45": {
    "doc": "Travel",
    "title": "Workshop Venue",
    "content": "The workshop will be held at the W Abu Dhabi Hotel at Yas Island. A block of rooms is available for reservation to workshop attendees: . Book a room at workshop rates . The details are as follows: . | Reservation start date: Tuesday, January 3, 2023 | Reservation end date: Saturday, January 7, 2023 | Last day to book: Friday, December 23, 2022 | . Authors of accepted papers will be contacted about finding a roommate to enjoy the double occupancy rate, if they wish. For other attendees interested in seeking a roommate, please email the organizers. ",
    "url": "/travel/#workshop-venue",
    "relUrl": "/travel/#workshop-venue"
  },"46": {
    "doc": "Travel",
    "title": "Travel FAQ for Authors",
    "content": "Can the travel grant be adjusted depending on location? . Unfortunately, we cannot adjust the amount of the travel grant based on locations. I will not be able to attend on Jan. 3rd. The poster sessions will be on Jan. 4th – 6th, so please make sure that you will be able to attend at least one of the poster sessions. Can I get visa support? . MBZUAI does not routinely provide visa support, however all relevant information can be found here. ",
    "url": "/travel/#travel-faq-for-authors",
    "relUrl": "/travel/#travel-faq-for-authors"
  },"47": {
    "doc": "Travel",
    "title": "Travel",
    "content": "Third Workshop on Seeking Low&#8209;Dimensionality in Deep&nbsp;Neural&nbsp;Networks January 2023,&nbsp;MBZUAI ",
    "url": "/travel/",
    "relUrl": "/travel/"
  },"48": {
    "doc": "Tutorials",
    "title": "Tutorials",
    "content": "The first day of the workshop features tutorial presentations from a subset of the organizers. These tutorials present an up-to-date account of the intersection between low-dimensional modeling and deep learning in an accessible format. The tutorials are summarized below. See the schedule for the precise times of each tutorial. Some of the tutorials draw on material from an ICASSP 2022 short course. Introduction to Low-Dimensional Models . John Wright . Columbia University . The first session will introduce fundamental properties and theoretical results for sensing, processing, analyzing, and learning low-dimensional structures from high-dimensional data. We will first discuss classical low-dimensional models, such as sparse recovery and low-rank matrix sensing, and motivate these models by applications in medical imaging, collaborative filtering, face recognition, and beyond. Based on convex relaxation, we will characterize the conditions, in terms of sample/data complexity, under which the inverse problems of recovering such low-dimensional structures become tractable and can be solved efficiently, with guaranteed correctness or accuracy. Nonconvex Optimization of Low-Dimensional Models . Yuqian Zhang . Rutgers University . We will transit from sensing to learning low-dimensional structures, such as dictionary learning, sparse blind deconvolution, and dual principal component analysis. Problems associated with learning low-dimensional models from sample data are often nonconvex: either they do not have tractable convex relaxations or the nonconvex formulation is preferred due to physical or computational constraints (such as limited memory). To deal with these challenges, we will introduce a systematic approach of analyzing the corresponding nonconvex landscapes from a geometry and symmetry perspective. The resulting approach leads to provable globally convergent nonconvex optimization methods. Learning Low-Dimensional Structure via Deep Networks . Sam Buchanan . TTIC . We will discuss the contemporary topic of using deep models for computing with nonlinear data, introducing strong conceptual connections between low-dimensional structures in data and deep models. We will then consider a mathematical model problem that attempts to capture these aspects of practice, and show how low-dimensional structure in data and tasks influences the resources (statistical, architectural) required to achieve a given performance level. Our discussion will revolve around basic tradeoffs between these resources and theoretical guarantees of performance. Learned Representations and Low-Dimensional Structures . Zhihui Zhu . Ohio State University . Continuing our exploration of deep models for nonlinear data, we will begin to delve into learned representations, network architectures, regularizations, and beyond. We will see how the tools for nonconvexity developed previously shed light on the learned representations produced by deep networks, through connections to matrix factorization. We will observe how algorithms that interact with data will expose additional connections to low-dimensional models, through implicit regularization of the network parameters. Design Deep Networks for Pursuing Low-Dimensional Structures . Yi Ma . UC Berkeley . Based upon the previous discussion on the connection between low-dimensional structures and deep models, in this section, we will discuss principles for designing deep networks through the lens of learning good low-dimensional representation for (potentially nonlinear) low-dimensional structures. We will see how unrolling iterative optimization algorithms for low-dimensional problems (such as the sparsifying algorithms) naturally lead to deep neural networks. We will then show how modern deep layered architectures, linear (convolution) operators, and nonlinear activations, and even all parameters can be derived from the principle of learning a compact linear discriminative representation for nonlinear low-dimensional structures within the data. We will show how so learned representations can bring tremendous benefits in tasks such as learning generative models, noise stability, and incremental learning. Sparsity Neural Networks – Practice and Theory . Atlas Wang . UT Austin . We discuss the role of sparsity in general neural network architectures, and shed light on how sparsity interacts with deep learning under the overparameterization regime, for both practitioners and theorists. A sparse neural network (NN) has most of its parameters set to zero and is traditionally considered as the product of NN compression (i.e., pruning). Yet recently, sparsity has exposed itself as an important bridge for modeling the underlying low dimensionality of NNs, for understanding their generalization, optimization dynamics, implicit regularization, expressivity, and robustness. Deep NNs learned with sparsity-aware priors have also demonstrated significantly improved performances through a full stack of applied work on algorithms, systems, and hardware. In this talk, I plan to cover recent progress on the practical, theoretical, and scientific aspects of sparse NNs. I will try scratching the surface of three aspects – (1) practically, why one should love a sparse NN, beyond just a post-training NN compression tool; (2) theoretically, what are some guarantees that one can expect from sparse NNs; and (3) what is future prospect of exploiting sparsity in NNs. Advancing Machine Learning for Imaging – Regularization and Robustness . Saiprasad Ravishankar . Michigan State University . In this talk, we present our work on improving machine learning for image reconstruction on three fronts – i) learning regularizers, ii) learning with no training data, and iii) ensuring robustness to perturbations in learning-based schemes. First, we present an approach for supervised learning of sparsity-promoting regularizers, where the parameters of the regularizer are learned to minimize reconstruction error on a paired training set. Training involves a challenging bilevel optimization problem with a nonsmooth lower-level objective. We derive an expression for the gradient of the training loss using the implicit closed-form solution of the lower-level variational problem, and provide an accompanying exact gradient descent algorithm (dubbed BLORC). Our experiments show that the gradient computation is efficient and BLORC learns meaningful operators for effective denoising. Second, we investigate the deep image prior (DIP) scheme that recovers an image by fitting an overparameterized neural network directly to the image’s corrupted measurements. To address DIP’s overfitting and performance issues, recent work proposed using a reference image as the network input. However, obtaining the reference often requires supervision. Hence, we propose a self-guided scheme that uses only undersampled measurements to estimate both the network weights and input image. We exploit regularization requiring the network to be a powerful denoiser. Our self-guided method gives significantly improved reconstructions for MRI with limited measurements compared to recent schemes, while using no training data. Finally, recent studies have shown that trained deep reconstruction models could be over-sensitive to tiny input perturbations, which cause unstable, low-quality reconstructed images. To address this issue, we propose Smoothed Unrolling (SMUG), which incorporates a randomized smoothing-based robust learning operation into a deep unrolling architecture and improves the robustness of MRI reconstruction with respect to diverse perturbations. ",
    "url": "/tutorials/#tutorials",
    "relUrl": "/tutorials/#tutorials"
  },"49": {
    "doc": "Tutorials",
    "title": "Tutorials",
    "content": "Third Workshop on Seeking Low&#8209;Dimensionality in Deep&nbsp;Neural&nbsp;Networks January 2023,&nbsp;MBZUAI ",
    "url": "/tutorials/",
    "relUrl": "/tutorials/"
  }
}
