{"0": {
    "doc": "Home",
    "title": "Key Dates and Deadlines",
    "content": ". | Workshop dates: 3rd - 7th January, 2023; in person at W Hotel, Abu Dhabi | Submission portal opens: October 1st, 2022 | Paper submission deadline: November 6th, 2022 | Acceptance and travel grant notification: November 20th, 2022 | . The top accepted papers will be recommended for inclusion in a future special issue of the IEEE Journal of Selected Topics in Signal Processing. ",
    "url": "/#key-dates-and-deadlines",
    "relUrl": "/#key-dates-and-deadlines"
  },"1": {
    "doc": "Home",
    "title": "Workshop Themes",
    "content": "In the past decade, deep learning has demonstrated unprecedented performance across many different domains. Modern neural networks learn high-performing nonlinear representations for complex multi-modal data (e.g. text and image) without labels; they generate near-photorealistic high-resolution images from random noise; and they efficiently reconstruct data from compressive, corrupted measurements in various imaging and structural prediction tasks. However, despite recent endeavors, the underlying principles behind their success remain shrouded in mystery. At a fundamental level, each of these successes, and numerous others in scientific and engineering applications, stem from the low-dimensional structure present both in the training data and in the networks themselves. This presents a significant opportunity to bring insights from better-understood low-dimensional models to the setting of deep learning, where models are notoriously plagued by issues of poor resource and data efficiency, robustness, and generalization. Given these exciting but relatively less-exploited connections, this workshop aims to bring together experts in machine learning, applied mathematics, signal processing, and optimization, to share recent progress and foster collaborations on the mathematical foundations of deep learning. As the community continues to embrace the power of deep learning, with unprecedented new challenges in terms of modeling and interpretability, we hope to stimulate vibrant discussions towards bridging the gap between the theory and practice of deep learning, with low-dimensionality as a unifying focus. ",
    "url": "/#workshop-themes",
    "relUrl": "/#workshop-themes"
  },"2": {
    "doc": "Home",
    "title": "Confirmed Speakers",
    "content": "Information on the speakers’ planned talks is available here. <!-- ",
    "url": "/#confirmed-speakers",
    "relUrl": "/#confirmed-speakers"
  },"3": {
    "doc": "Home",
    "title": "Confirmed Distinguished Speakers",
    "content": "--> Misha Belkin . UC San Diego . Michael Bronstein . University of Oxford . Beidi Chen . Meta . Ivan Dokmanić . University of Basel . Simon Du . University of Washington . Michael Elad . Technion . Reinhard Heckel . TU Munich . Gitta Kutyniok . LMU Munich . Jason Lee . Princeton . Qi Lei . NYU . Rina Panigrahy . Google . Tomaso Poggio . MIT . Ohad Shamir . Weizmann Institute . Mahdi Soltanolkotabi . USC . Daniel Soudry . Technion . Weijie Su . UPenn . René Vidal . Johns Hopkins . Yu-Xiang Wang . UCSB . Bihan Wen . NTU . Fanny Yang . ETH Zurich . ",
    "url": "/",
    "relUrl": "/"
  },"4": {
    "doc": "Home",
    "title": "Workshop Schedule (Tentative)",
    "content": "All times in GST (GMT+4). ",
    "url": "/#workshop-schedule-tentative",
    "relUrl": "/#workshop-schedule-tentative"
  },"5": {
    "doc": "Home",
    "title": "",
    "content": "Tuesday, January 3 Welcome Session and Educational Tutorials . | Tutorial on recent progress in low-dimensional models for high-dimensional data, based on an ICASSP 2022 short course with additional material on sparse neural networks and on self-expressive models. | . Wednesday, January 4 Invited Talks and Poster Presentations . | Plenaries and Invited Talks | Coffee breaks for discussion | Afternoon poster session | . Thursday, January 5 Invited Talks and Poster Presentations . | Plenaries and Invited Talks | Coffee breaks for discussion | Afternoon poster session | . Friday, January 6 Invited Talks and Panel Discussion . | Plenaries and Invited Talks | Coffee breaks for discussion | Afternoon panel discussion (additional details TBA) | . Saturday, January 7 Social Events . | Local tour of Abu Dhabi or day tour of Dubai for networking | . ",
    "url": "/",
    "relUrl": "/"
  },"6": {
    "doc": "Home",
    "title": "Organizers",
    "content": "Yi Ma . UC Berkeley . General Chair . Eric P. Xing . MBZUAI . General Chair (Local) . Samuel Horvath . MBZUAI . Local Chair . Karthik Nandakumar . MBZUAI . Local Chair . Martin Takáč . MBZUAI . Local Chair . Qing Qu . University of Michigan . Program Chair and Publication Chair . Jeremias Sulam . Johns Hopkins University . Program Chair . Atlas Wang . UT Austin . Program Chair . John Wright . Columbia University . Program Chair . Yuqian Zhang . Rutgers University . Program Chair . Yuejie Chi . Carnegie Mellon University . Publicity Chair . Chong You . Google NYC . Publication Chair . Zhihui Zhu . Ohio State University . Publication Chair . Sam Buchanan . TTIC . Tutorial Chair . Saiprasad Ravishankar . Michigan State University . Online Chair . ",
    "url": "/#organizers",
    "relUrl": "/#organizers"
  },"7": {
    "doc": "Home",
    "title": "Host Institution",
    "content": " ",
    "url": "/",
    "relUrl": "/"
  },"8": {
    "doc": "Home",
    "title": "Home",
    "content": "Third Workshop on Seeking Low&#8209;Dimensionality in Deep&nbsp;Neural&nbsp;Networks January 2023,&nbsp;MBZUAI Join Us in Abu Dhabi from 3-7 January 2023! . Announcing the workshop venue: W Abu Dhabi Hotel at Yas Island . Register here for in-person and remote participation . ",
    "url": "/",
    "relUrl": "/"
  },"9": {
    "doc": "Announcements",
    "title": "Announcements",
    "content": "Announcements are stored in the _announcements directory and rendered according to the layout file, _layouts/announcement.html. ",
    "url": "/announcements/",
    "relUrl": "/announcements/"
  },"10": {
    "doc": "Announcements",
    "title": "Week 1 Announcement",
    "content": "Apr 8 &middot; 0 min read . | Create a new repository based on Just the Class. | Configure a publishing source for GitHub Pages. Your course website is now live! | Update _config.yml with your course information. | Edit and create .md Markdown files to add your content. | . ",
    "url": "/announcements/",
    "relUrl": "/announcements/"
  },"11": {
    "doc": "Announcements",
    "title": "Week 0 Announcement",
    "content": "Apr 1 &middot; 0 min read Hello world! . ",
    "url": "/announcements/",
    "relUrl": "/announcements/"
  },"12": {
    "doc": "Calendar",
    "title": "Calendar",
    "content": ". | 8:00 AM | 8:30 AM | 9:00 AM | 9:30 AM | 10:00 AM | 10:30 AM | 11:00 AM | 11:30 AM | 12:00 PM | 12:30 PM | 1:00 PM | 1:30 PM | 2:00 PM | 2:30 PM | 3:00 PM | 3:30 PM | 4:00 PM | 4:30 PM | 5:00 PM | 5:30 PM | . | ",
    "url": "/calendar/",
    "relUrl": "/calendar/"
  },"13": {
    "doc": "Calendar",
    "title": "Tuesday, Jan 3",
    "content": ". | Opening Ceremony 8:30 AM–9:00 AM | asdf 11:30 AM–12:30 PM 310 Soda | Office Hours 12:30 PM–2:00 PM 271 Soda | . | ",
    "url": "/calendar/",
    "relUrl": "/calendar/"
  },"14": {
    "doc": "Calendar",
    "title": "Wednesday",
    "content": ". | Lecture 9:30 AM–10:30 AM 150 Wheeler | Section 11:30 AM–12:30 PM 310 Soda | Office Hours 12:30 PM–2:00 PM 271 Soda | . | ",
    "url": "/calendar/",
    "relUrl": "/calendar/"
  },"15": {
    "doc": "Calendar",
    "title": "Thursday",
    "content": "| ",
    "url": "/calendar/",
    "relUrl": "/calendar/"
  },"16": {
    "doc": "Calendar",
    "title": "Friday",
    "content": ". | Lecture 9:30 AM–10:30 AM 150 Wheeler | Section 11:30 AM–12:30 PM 310 Soda | Office Hours 12:30 PM–2:00 PM 271 Soda | . | ",
    "url": "/calendar/",
    "relUrl": "/calendar/"
  },"17": {
    "doc": "Calendar",
    "title": "Saturday, Jan 7",
    "content": "| . ",
    "url": "/calendar/",
    "relUrl": "/calendar/"
  },"18": {
    "doc": "Organizers",
    "title": "Organizers",
    "content": "Yi Ma . UC Berkeley . General Chair . Eric P. Xing . MBZUAI . General Chair (Local) . Samuel Horvath . MBZUAI . Local Chair . Karthik Nandakumar . MBZUAI . Local Chair . Martin Takáč . MBZUAI . Local Chair . Qing Qu . University of Michigan . Program Chair and Publication Chair . Jeremias Sulam . Johns Hopkins University . Program Chair . Atlas Wang . UT Austin . Program Chair . John Wright . Columbia University . Program Chair . Yuqian Zhang . Rutgers University . Program Chair . Yuejie Chi . Carnegie Mellon University . Publicity Chair . Chong You . Google NYC . Publication Chair . Zhihui Zhu . Ohio State University . Publication Chair . Sam Buchanan . TTIC . Tutorial Chair . Saiprasad Ravishankar . Michigan State University . Online Chair . ",
    "url": "/organizers/#organizers",
    "relUrl": "/organizers/#organizers"
  },"19": {
    "doc": "Organizers",
    "title": "Organizers",
    "content": "Third Workshop on Seeking Low&#8209;Dimensionality in Deep&nbsp;Neural&nbsp;Networks January 2023,&nbsp;MBZUAI ",
    "url": "/organizers/",
    "relUrl": "/organizers/"
  },"20": {
    "doc": "Past workshops",
    "title": "Past workshops",
    "content": ". | 2021 | 2020 | . ",
    "url": "/past_workshops/#past-workshops",
    "relUrl": "/past_workshops/#past-workshops"
  },"21": {
    "doc": "Past workshops",
    "title": "Past workshops",
    "content": "Third Workshop on Seeking Low&#8209;Dimensionality in Deep&nbsp;Neural&nbsp;Networks January 2023,&nbsp;MBZUAI ",
    "url": "/past_workshops/",
    "relUrl": "/past_workshops/"
  },"22": {
    "doc": "Schedule",
    "title": "Workshop Schedule (Tentative)",
    "content": "All times in GST (GMT+4). ",
    "url": "/schedule/#workshop-schedule-tentative",
    "relUrl": "/schedule/#workshop-schedule-tentative"
  },"23": {
    "doc": "Schedule",
    "title": "",
    "content": "Tuesday, January 3 Welcome Session and Educational Tutorials . | Tutorial on recent progress in low-dimensional models for high-dimensional data, based on an ICASSP 2022 short course with additional material on sparse neural networks and on self-expressive models. | . Wednesday, January 4 Invited Talks and Poster Presentations . | Plenaries and Invited Talks | Coffee breaks for discussion | Afternoon poster session | . Thursday, January 5 Invited Talks and Poster Presentations . | Plenaries and Invited Talks | Coffee breaks for discussion | Afternoon poster session | . Friday, January 6 Invited Talks and Panel Discussion . | Plenaries and Invited Talks | Coffee breaks for discussion | Afternoon panel discussion (additional details TBA) | . Saturday, January 7 Social Events . | Local tour of Abu Dhabi or day tour of Dubai for networking | . ",
    "url": "/schedule/",
    "relUrl": "/schedule/"
  },"24": {
    "doc": "Schedule",
    "title": "Schedule",
    "content": "Third Workshop on Seeking Low&#8209;Dimensionality in Deep&nbsp;Neural&nbsp;Networks January 2023,&nbsp;MBZUAI ",
    "url": "/schedule/",
    "relUrl": "/schedule/"
  },"25": {
    "doc": "Speakers",
    "title": "Confirmed Speakers",
    "content": "Clicking a speaker’s photo will jump to their talk information below. <!-- ",
    "url": "/speakers/#confirmed-speakers",
    "relUrl": "/speakers/#confirmed-speakers"
  },"26": {
    "doc": "Speakers",
    "title": "Confirmed Distinguished Speakers",
    "content": "--> Misha Belkin . UC San Diego . Michael Bronstein . University of Oxford . Beidi Chen . Meta . Ivan Dokmanić . University of Basel . Simon Du . University of Washington . Michael Elad . Technion . Reinhard Heckel . TU Munich . Gitta Kutyniok . LMU Munich . Jason Lee . Princeton . Qi Lei . NYU . Rina Panigrahy . Google . Tomaso Poggio . MIT . Ohad Shamir . Weizmann Institute . Mahdi Soltanolkotabi . USC . Daniel Soudry . Technion . Weijie Su . UPenn . René Vidal . Johns Hopkins . Yu-Xiang Wang . UCSB . Bihan Wen . NTU . Fanny Yang . ETH Zurich . ",
    "url": "/speakers/",
    "relUrl": "/speakers/"
  },"27": {
    "doc": "Speakers",
    "title": "Talk Details",
    "content": "Misha Belkin . UC San Diego . Title: Why do neural models need so many parameters? . Abstract . One of the striking aspects of modern neural networks is their extreme size reaching billions or even trillions parameters. Why are so many parameters needed? To attempt an answer to this question, I will discuss an algorithm and distribution independent non-asymptotic trade-off between the model size, excess test loss, and training loss for linear predictors and feature maps. Specifically, we show that models that perform well on the test data (have low excess loss) are either “classical” – have training loss close to the noise level, or are “modern” – have a much larger number of parameters compared to the minimum needed to fit the training data exactly. Furthermore, I will provide empirical evidence that optimal performance of realistic models is typically achieved in the “modern” regime, when they are trained below the noise level. Michael Bronstein . University of Oxford . Title: TBA . Abstract . TBA . Beidi Chen . Meta . Title: Hardware-aware Sparsity: Accurate and Efficient Foundation Model Training . Abstract . Foundation models trained on complex and rapidly growing data consume enormous computational resources. In this talk, I will describe our recent work on exploiting model and activation sparsity to accelerate foundation model training in both data and model parallel settings. We show that adapting algorithms on current hardware leads to efficient model training with no drop in accuracy. I will start by describing Pixelated Butterfly and Monarch, simple yet efficient sparse model training frameworks on GPUs. They use simple static block-sparse patterns based on butterfly and low-rank matrices, taking into account GPU block-oriented efficiency. They train up to 2.5x faster (wall-clock) than the dense Vision Transformer and GPT-2 counterparts with no drop in accuracy. Next, I will present AC-SGD, communication-efficient pipeline parallelism training frameworks over slow networks. Based on an interesting observation that model weights change slowly during the training, AC-SGD compresses activations or the change of activations with guarantees. It trains or fine-tunes DeBERTa and GPT2-1.5B 4.3x faster in slower networks without sacrificing model quality. I will conclude by outlining three future research directions - data efficiency,software-hardware codesign, and ML for science, and several ongoing projects including linear-time algorithm for large optimal transport problems, efficient autoregressive model (gpt3-style) inference, and ML for new material discovery. Ivan Dokmanić . University of Basel . Title: Statistical Mechanics of Graph Neural Networks . Abstract . Graph convolution networks are excellent models for relational data but their success is not well understood. I will show how ideas from statistical physics and random matrix theory allow us to precisely characterize GCN generalization on the contextual stochastic block model—a community-structured graph model with features. The resulting curves are rich: they predict double descent thus far unseen in graph learning and explain the qualitative distinction between learning on homophilic graphs (such as friendship networks) and heterophilic graphs (such as protein interaction networks). Earlier approaches based on VC-dimension or Rademacher complexity are too blunt to yield similar mechanistic insight. Our findings pleasingly translate to real “production-scale” networks and datasets and suggest simple redesigns which improve performance of state-of-the-art networks on heterophilic datasets. They further suggest intriguing connections with spectral graph theory, signal processing, and iterative methods for the Helmholtz equation. Joint work with Cheng Shi, Liming Pan, and Hong Hu. Simon Du . University of Washington . Title: Passive and Active Multi-Task Representation Learning . Abstract . Representation learning has been widely used in many applications. In this talk, I will present our work which uncovers when and why representation learning provably improves the sample efficiency, from a statistical learning point of view. Furthermore, I will talk about how to actively select the most relevant task to boost the performance. Michael Elad . Technion . Title: Image Denoising - Not What You Think . Abstract . Image denoising – removal of white additive Gaussian noise from an image – is one of the oldest and most studied problems in image processing. An extensive work over several decades has led to thousands of papers on this subject, and to many well-performing algorithms for this task. As expected, the era of deep learning has brought yet another revolution to this subfield, and took the lead in today’s ability for noise suppression in images. All this progress has led some researchers to believe that “denoising is dead”, in the sense that all that can be achieved is already done. Exciting as all this story might be, this talk IS NOT ABOUT IT! Our story focuses on recently discovered abilities and vulnerabilities of image denoisers. In a nut-shell, we expose the possibility of using image denoisers for serving other problems, such as regularizing general inverse problems and serving as the engine for image synthesis. We also unveil the (strange?) idea that denoising (and other inverse problems) might not have a unique solution, as common algorithms would have you believe. Instead, we will describe constructive ways to produce randomized and diverse high perceptual quality results for inverse problems. Reinhard Heckel . TU Munich . Title: The role of data and models for deep-learning based image reconstruction . Abstract . Deep-learning methods give state-of-the-art performance for a variety of imaging tasks, including accelerated magnetic resonance imaging. In this talk we discuss whether improved models and algorithms or training data are the most promising way forward. First, we ask whether increasing the model size and the training data improves performance in a similar fashion as it has in domains such as language modeling. We find that scaling beyond relatively few examples yields only marginal performance gains. Second, we discuss the robustness of deep learning based image reconstruction methods. Perhaps surprisingly, we find no evidence for neural networks being any less robust than classical reconstruction methods (such as l1 minimization). However, we find that both classical and deep learning based approaches perform significantly worse under distribution shifts, i.e., when trained (or tuned) and tested on slightly different data. Finally, we show that the out-of-distribution performance can be improved through more diverse training data, or through an algorithmic intervention called test-time-training. Gitta Kutyniok . LMU Munich . Title: The Next Generation of Reliable AI: From Digital to Analog Hardware . Abstract . Artificial intelligence is currently leading to one breakthrough after the other, in the sciences, in industry, and in public life. However, one current major drawback is the lack of reliability of such methodologies, which, in particular, also concerns almost any application of deep neural networks. In this lecture, we will first provide a short introduction into the world of reliability of deep neural networks, with one main focus being on explainability. We will, in particular, present a novel approach based on information theory, coined ShearletX, which allows to not only provide higher level explanations, but also reveals the reason for wrong decisions. We will then delve deeper and discuss fundamental limitations of numerous current deep learning-based approaches, showing that there do exist severe problems in terms of computability on any type of digital hardware, which seriously affects their reliability. But theory also shows a way out, pointing towards a future on analog hardware such as neuromorphic computing or quantum computing to achieve true reliability. Jason Lee . Princeton . Title: Feature Learning with gradient descent . Abstract . Significant theoretical work has established that in specific regimes, neural networks trained by gradient descent behave like kernel methods. However, in practice, it is known that neural networks strongly outperform their associated kernels. In this work, we explain this gap by demonstrating that there is a large class of functions which cannot be efficiently learned by kernel methods but can be easily learned with gradient descent on a two layer neural network outside the kernel regime by learning representations that are relevant to the target task. We also demonstrate that these representations allow for efficient transfer learning, which is impossible in the kernel regime. Specifically, we consider the problem of learning polynomials which depend on only a few relevant directions, i.e. of the form f⋆(x)=g(Ux) where U: \\R^d \\to \\R^r with d≫r. When the degree of f⋆ is p, it is known that n≍d^p samples are necessary to learn f⋆ in the kernel regime. Our primary result is that gradient descent learns a representation of the data which depends only on the directions relevant to f⋆. This results in an improved sample complexity of n≍rd^2+pd^r. Furthermore, in a transfer learning setup where the data distributions in the source and target domain share the same representation U but have different polynomial heads we show that a popular heuristic for transfer learning has a target sample complexity independent of d. This is joint work with Alex Damian and Mahdi Soltanolkotabi. Qi Lei . NYU . Title: Reconstructing Training Data from Model Gradient, Provably . Abstract . Understanding when and how much a model gradient leaks information about the training sample is an important question in privacy. In this talk, we present a surprising result: even without training or memorizing the data, we can fully reconstruct the training samples from a single gradient query at a randomly chosen parameter value. We prove the identifiability of the training data under mild conditions: with shallow or deep neural networks and a wide range of activation functions. We also present a statistically and computationally efficient algorithm based on low-rank tensor decomposition to reconstruct the training data. As a provable attack that reveals sensitive training data, our findings suggest potential severe threats to privacy, especially in federated learning. Rina Panigrahy . Google . Title: How to learn a Table of Concepts? . Abstract . An idealized view of an intelligent system is one where there is a table of concepts from simple to complex and on each raw input (image or text) is processed to identify the concepts present or triggered by that input. While today’s ML systems may use a Mixture of Experts or Table of entities/mentions that are trying to map concepts to experts/entities/mention, there is hardly a clear picture of how concepts build upon each other automatically. In the domain of images, we can think of commonly occurring shapes, types of motions as concepts and ask how would a visual system create entries for different shape types in a table of concepts. How does one convert raw data into concepts given by something like their latent representation. A trivial transformation like random projection doesn’t work. But we will see how simple networks similar to ones used in practice can be viewed as “sketching operators” that can be used to create representations for images with simple shapes that are isomorphic to their polynomial representations. By putting such polynomial representations in a locality sensitive table, we obtain a dictionary of curves. And then by recursively applying another layer of the sketching operator on the curves one gets a dictionary of concepts such as shapes. Tomaso Poggio . MIT . Title: A key principle underlying deep networks: compositional sparsity . Abstract . A key question is whether there exist a theoretical explanation — a common motif — to the various network architectures, including the human brain, that perform so well in learning tasks. I will discuss the conjecture that this is compositional sparsity of effectively computable functions: all functions of many variables must effectively be compositionally sparse that is with constituent functions each depending on a small number of variables. Ohad Shamir . Weizmann Institute . Title: Implicit bias in machine learning . Abstract . Most practical algorithms for supervised machine learning boil down to optimizing the average performance over a training dataset. However, it is increasingly recognized that although the optimization objective is the same, the manner in which it is optimized plays a decisive role in the properties of the resulting predictor. For example, when training large neural networks, there are generally many weight combinations that will perfectly fit the training data. However, gradient-based training methods somehow tend to reach those which, for example, do not overfit; are brittle to adversarially crafted examples; or have other unusual properties. In this talk, I’ll describe several recent theoretical and empirical results related to this question. Mahdi Soltanolkotabi . USC . Title: Demystifying Feature learning via gradient descent with applications to medical image reconstruction . Abstract . In this talk I will discuss the challenges and opportunities for using deep learning in medical image reconstruction. Contemporary techniques in this field rely on convolutional architectures that are limited by the spatial invariance of their filters and have difficulty modeling long-range dependencies. To remedy this, I will discuss our work on designing new transformer-based architectures called HUMUS-Net that lead to state of the art performance and do not suffer from these limitations. A key component in the success of the above approach is a unique feature learning capability of unrolled neural networks trained based on end-to-end training. In the second part of the talk I will demystify this feature learning capability of neural networks in this context as well as more broadly for other problems. Our result is based on an intriguing spectral bias phenomena for gradient descent, that puts the iterations on a particular trajectory towards solutions that learn good features that generalize well. Notably this analysis overcomes a major theoretical bottleneck in the existing literature and goes beyond the “lazy” training regime which requires unrealistic hyperparameter choices (e.g. very small step sizes, large initialization or wide models). Daniel Soudry . Technion . Title: How catastrophic can catastrophic forgetting be in linear regression? . Abstract . Deep neural nets typically forget old tasks when trained on new tasks. This phenomenon, called “catastrophic forgetting” is not well understood, even in the most basic setting of linear regression. Therefore, we study catastrophic forgetting when fitting an overparameterized linear model to a sequence of tasks with different input distributions. We analyze how much the model forgets the true labels of earlier tasks after training on subsequent tasks, obtaining exact expressions and bounds. We establish connections between continual learning in the linear setting and two other research areas: alternating projections and the Kaczmarz method. In specific settings, we highlight differences between forgetting and convergence to the offline solution as studied in those areas. In particular, when T tasks in d dimensions are presented cyclically for k iterations, we prove an upper bound of T^2 * min{1/sqrt(k), d/k} on the forgetting. This stands in contrast to the convergence to the offline solution, which can be arbitrarily slow according to existing alternating projection results. We further show that the T^2 factor can be lifted when tasks are presented in a random ordering. Joint work with Itay Evron, Edward Moroshko, Rachel Ward, and Nati Srebro, published in COLT 22. Weijie Su . UPenn . Title: Geometrization of Real-World Deep Neural Networks . Abstract . In this talk, we will investigate the emergence of geometric patterns in well-trained deep learning models by making use of a layer-peeled model and the law of equi-separation. The former is a nonconvex optimization program that models the last-layer features and weights. We use the model to shed light on the neural collapse phenomenon of Papyan, Han, and Donoho, and to predict a hitherto-unknown phenomenon that we term minority collapse in imbalanced training. This is based on joint work with Cong Fang, Hangfeng He, and Qi Long (arXiv:2101.12699). The law of equi-separation is a pervasive empirical phenomenon that describes how data are separated according to their class membership from the bottom to the top layer in a well-trained neural network. We will show that, through extensive computational experiments, neural networks improve data separation through layers in a simple exponential manner. This law leads to roughly equal ratios of separation that a single layer is able to improve, thereby showing that all layers are created equal. We will conclude the talk by discussing the implications of this law on the interpretation, robustness, and generalization of deep learning, as well as on the inadequacy of some existing approaches toward demystifying deep learning. This is based on joint work with Hangfeng He (arXiv:2210.17020). René Vidal . Johns Hopkins . Title: Principled Defenses and Reverse Engineering of Adversarial Attacks in a Union of Subspaces . Abstract . Deep neural network-based classifiers have been shown to be vulnerable to imperceptible perturbations to their input, such as ℓp-bounded norm adversarial attacks. This has motivated the development of many defense methods, which are then broken by new attacks, and so on. Recent work has also focused on the problem of reverse engineering adversarial attacks, which requires both recovering the clean signal and determining the type of attack (ℓ1, ℓ2 or ℓ∞). However, existing methods either do not come with provable guarantees, or they can certify the accuracy of the classifier only for very small perturbations. In this work, we assume that the data lies approximately in a union of low-dimensional linear subspaces and exploit this low-dimensional structure to develop a theory of adversarial robustness for subspace-sparse classifiers. We first derive geometric conditions on the subspaces under which any attacked signal can be decomposed as the sum of a clean signal plus an attack. We then derive norm-independent certification regions, showing that we can provably defend against specific unrestricted adversarial attacks. Yu-Xiang Wang . UCSB . Title: Deep Learning meets Nonparametric Regression: Are Weight-decayed DNNs locally adaptive? . Abstract . They say deep learning is just curve fitting. But how good is it in curve fitting exactly? Are DNNs as good as, or even better than, classical curve-fitting tools, e.g., splines and wavelets? In this talk, I will cover my group’s recent paper on this topic and some interesting insight. Specifically, I will provide new answers to “Why is DNN stronger than kernels?” “Why are deep NNs stronger than shallow ones?”, “Why ReLU?”, “How do DNNs generalize under overparameterization?”, “What is the role of sparsity in deep learning?”, “Is lottery ticket hypothesis real?” All in one package. Intrigued? Come to my talk and find out! . Bihan Wen . NTU . Title: Beyond Classic Image Priors: Deep Reinforcement Learning and Disentangling for Image Restoration . Abstract . The key to solving various ill-posed inverse problems, including many computational imaging and image restoration tasks, is to exploit effective image priors. The classic signal processing theory laid the foundation on constructing analytical image models such as sparse coding and low-rank approximation. Recent advances in machine learning, especially deep learning technologies have made incredible progress in the past few years, which enables people to rethink how the image prior can be formulated more effective. Despite those many deep learning methods achieved the state-of-the-art results in image restoration tasks, there are still limitations in practice, such as adversarial robustness, customization, generalization, and data-efficiency. In this talk, I will share some of our recent works on deep disentangling and deep reinforcement learning in image restoration tasks. We argue that learning more than just the classic image priors are needed for solving inverse problems to alleviate some of the limitations. We show promising results in several image restoration tasks, including denoising, adversarial purification, low-light image enhancement and computational imaging. Fanny Yang . ETH Zurich . Title: Strong inductive biases provably prevent harmless interpolation . Abstract . Interpolating models have recently gained popularity in the statistical learning community due to common practices in modern machine learning: complex models achieve good generalization performance despite interpolating high-dimensional training data. In this talk, we prove generalization bounds for high-dimensional linear models that interpolate noisy data generated by a sparse ground truth. In particular, we first show that minimum-l1-norm interpolators achieve high-dimensional asymptotic consistency at a logarithmic rate. Further, as opposed to the regularized or noiseless case, for min-lp-norm interpolators with 1&lt;p&lt;2 we surprisingly obtain polynomial rates. Our results suggest a new trade-off for interpolating models: a stronger inductive bias encourages a simpler structure better aligned with the ground truth at the cost of an increased variance. We finally discuss our latest results where we show that this phenomenon also holds for nonlinear models. ",
    "url": "/speakers/#talk-details",
    "relUrl": "/speakers/#talk-details"
  },"28": {
    "doc": "Speakers",
    "title": "Speakers",
    "content": "Third Workshop on Seeking Low&#8209;Dimensionality in Deep&nbsp;Neural&nbsp;Networks January 2023,&nbsp;MBZUAI ",
    "url": "/speakers/",
    "relUrl": "/speakers/"
  },"29": {
    "doc": "Host Institution",
    "title": "Host Institution",
    "content": " ",
    "url": "/sponsors/",
    "relUrl": "/sponsors/"
  },"30": {
    "doc": "Host Institution",
    "title": "Host Institution",
    "content": "Third Workshop on Seeking Low&#8209;Dimensionality in Deep&nbsp;Neural&nbsp;Networks January 2023,&nbsp;MBZUAI ",
    "url": "/sponsors/",
    "relUrl": "/sponsors/"
  },"31": {
    "doc": "Call for Papers",
    "title": "Key Dates",
    "content": ". | Workshop dates: 3rd - 7th January, 2023; in person at W Hotel, Abu Dhabi | Submission portal opens: October 1st, 2022 | Paper submission deadline: November 6th, 2022 | Acceptance and travel grant notification: November 20th, 2022 | . ",
    "url": "/submission/#key-dates",
    "relUrl": "/submission/#key-dates"
  },"32": {
    "doc": "Call for Papers",
    "title": "How to Submit Your Work",
    "content": "We will use OpenReview to manage submissions. Submit your work here: . https://openreview.net/group?id=mbzuai.ac.ae/SLowDNN/2023/Workshop . See the submission guidelines and topics of interest. ",
    "url": "/submission/#how-to-submit-your-work",
    "relUrl": "/submission/#how-to-submit-your-work"
  },"33": {
    "doc": "Call for Papers",
    "title": "Logistics for Accepted Papers",
    "content": "Accepted works will be expected to present a poster describing the work in-person at the workshop. Travel grants are available to support authors of accepted papers: see the travel page for details. A small subset of the top accepted papers will be recommended for inclusion in a future special issue of the IEEE Journal of Selected Topics in Signal Processing. ",
    "url": "/submission/#logistics-for-accepted-papers",
    "relUrl": "/submission/#logistics-for-accepted-papers"
  },"34": {
    "doc": "Call for Papers",
    "title": "Topics of Interest",
    "content": "Topics of interest include, but are not limited to, connections between low-dimensional models and the theory, architectures, algorithms, and applications of deep neural networks: . | Theory: approximation, generalization, robustness, compact and structured representations | Optimization: Benign non-convex optimization, implicit bias analysis, convergence guarantees | Architectures: compact/model-based/neuro-inspired/invariant neural networks | Algorithms: pruning, sparse training, robust training | Applications: generative models, resource/data-efficient learning, inverse problems | . ",
    "url": "/submission/#topics-of-interest",
    "relUrl": "/submission/#topics-of-interest"
  },"35": {
    "doc": "Call for Papers",
    "title": "Submission Guidelines",
    "content": "We aim to showcase: . | The latest research innovations at all stages of the research process, from work-in-progress to recently published papers | Position or survey papers on any topics relevant to this workshop (see the call above) | . Concretely, we ask members of the community to submit a conference-style paper (from four to eight pages, with extra pages for references) describing the work. Please also upload a short (250 word) abstract to OpenReview. Do not anonymize submissions. Papers should be written using the NeurIPS 2022 style files, available here. OpenReview submissions may also include any of the following supplemental materials that describe the work in further detail. | A poster (in PDF form) presenting results of work-in-progress. | A link to a blog post (e.g., distill.pub, Medium) describing results. | Appendices with detailed derivations and additional experiments. | . This workshop is non-archival, and it will not have proceedings. We permit under-review or concurrent submissions. Reviewing will be performed in a single-blind fashion (authors should not anonymize their submissions). ",
    "url": "/submission/#submission-guidelines",
    "relUrl": "/submission/#submission-guidelines"
  },"36": {
    "doc": "Call for Papers",
    "title": "Call for Papers",
    "content": "Third Workshop on Seeking Low&#8209;Dimensionality in Deep&nbsp;Neural&nbsp;Networks January 2023,&nbsp;MBZUAI ",
    "url": "/submission/",
    "relUrl": "/submission/"
  },"37": {
    "doc": "Travel",
    "title": "Travel Support",
    "content": "MBZUAI has travel grants in the amount of up to $1000 USD available for up to forty authors of accepted papers. All accepted papers will be considered for a travel grant. Accepted papers are expected to be presented in-person. To submit work, see the submission page. Travel grants will be announced concurrently with paper acceptances. ",
    "url": "/travel/#travel-support",
    "relUrl": "/travel/#travel-support"
  },"38": {
    "doc": "Travel",
    "title": "Workshop Venue",
    "content": "The workshop will be held at the W Abu Dhabi Hotel at Yas Island. A block of rooms is available for reservation to workshop attendees: . Book a room at workshop rates . The details are as follows: . | Reservation start date: Tuesday, January 3, 2023 | Reservation end date: Saturday, January 7, 2023 | Last day to book: Friday, December 23, 2022 | . Authors of accepted papers will be contacted about finding a roommate to enjoy the double occupancy rate, if they wish. For other attendees interested in seeking a roommate, please email the organizers. ",
    "url": "/travel/#workshop-venue",
    "relUrl": "/travel/#workshop-venue"
  },"39": {
    "doc": "Travel",
    "title": "Travel FAQ for Authors",
    "content": "Can the travel grant be adjusted depending on location? . Unfortunately, we cannot adjust the amount of the travel grant based on locations. I will not be able to attend on Jan. 3rd. The poster sessions will be on Jan. 4th – 6th, so please make sure that you will be able to attend at least one of the poster sessions. Can I get visa support? . MBZUAI does not routinely provide visa support, however all relevant information can be found here. ",
    "url": "/travel/#travel-faq-for-authors",
    "relUrl": "/travel/#travel-faq-for-authors"
  },"40": {
    "doc": "Travel",
    "title": "Travel",
    "content": "Third Workshop on Seeking Low&#8209;Dimensionality in Deep&nbsp;Neural&nbsp;Networks January 2023,&nbsp;MBZUAI ",
    "url": "/travel/",
    "relUrl": "/travel/"
  }
}
