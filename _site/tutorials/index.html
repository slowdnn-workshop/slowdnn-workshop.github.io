<!DOCTYPE html><html lang="en-US"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=Edge"><link rel="stylesheet" href="/assets/css/just-the-docs-default.css"> <script src="/assets/js/just-the-docs.js"></script><meta name="viewport" content="width=device-width, initial-scale=1"><title>Tutorials | SLowDNN Workshop</title><meta name="generator" content="Jekyll v3.9.2" /><meta property="og:title" content="Tutorials" /><meta property="og:locale" content="en_US" /><meta name="description" content="A listing of the tutorials that will be presented the first day of the workshop." /><meta property="og:description" content="A listing of the tutorials that will be presented the first day of the workshop." /><link rel="canonical" href="http://localhost:4000/tutorials/" /><meta property="og:url" content="http://localhost:4000/tutorials/" /><meta property="og:site_name" content="SLowDNN Workshop" /><meta property="og:type" content="website" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Tutorials" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"WebPage","description":"A listing of the tutorials that will be presented the first day of the workshop.","headline":"Tutorials","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"http://localhost:4000/assets/images/logo1.png"}},"url":"http://localhost:4000/tutorials/"}</script><body> <a class="skip-to-main" href="#main-content">Skip to main content</a> <svg xmlns="http://www.w3.org/2000/svg" style="display: none;"> <symbol id="svg-link" viewBox="0 0 24 24"><title>Link</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"><title>Search</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"><title>Menu</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"><title>Expand</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"><polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"><title>Document</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"><path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> <symbol id="svg-external-link" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link"><title id="svg-external-link-title">(external link)</title><path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line> </symbol> <symbol id="svg-copy" viewBox="0 0 16 16"><title>Copy</title><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16"><path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1v-1z"/><path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3z"/> </svg> </symbol> <symbol id="svg-copied" viewBox="0 0 16 16"><title>Copied</title><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard-check-fill" viewBox="0 0 16 16"><path d="M6.5 0A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3Zm3 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3Z"/><path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1A2.5 2.5 0 0 1 9.5 5h-3A2.5 2.5 0 0 1 4 2.5v-1Zm6.854 7.354-3 3a.5.5 0 0 1-.708 0l-1.5-1.5a.5.5 0 0 1 .708-.708L7.5 10.793l2.646-2.647a.5.5 0 0 1 .708.708Z"/> </svg> </symbol> </svg><div class="side-bar"><div class="site-header"> <a href="/" class="site-title lh-tight"><div class="site-logo"></div></a> <a href="#" id="menu-button" class="site-button"> <svg viewBox="0 0 24 24" class="icon"><use xlink:href="#svg-menu"></use></svg> </a></div><nav aria-label="Main" id="site-nav" class="site-nav"><ul class="nav-list"><li class="nav-list-item"><a href="/" class="nav-list-link">Home</a><li class="nav-list-item"><a href="/schedule/" class="nav-list-link">Schedule</a><li class="nav-list-item"><a href="/speakers/" class="nav-list-link">Speakers</a><li class="nav-list-item"><a href="/posters/" class="nav-list-link">Accepted Papers</a><li class="nav-list-item active"><a href="/tutorials/" class="nav-list-link active">Tutorials</a><li class="nav-list-item"><a href="/travel/" class="nav-list-link">Travel</a><li class="nav-list-item"><a href="/organizers/" class="nav-list-link">Organizers</a><li class="nav-list-item"><a href="/sponsors/" class="nav-list-link">Host Institution</a><li class="nav-list-item"><a href="/submission/" class="nav-list-link">Call for Papers</a><li class="nav-list-item"><a href="/past_workshops/" class="nav-list-link">Past workshops</a></ul></nav><footer class="site-footer"> For inquiries about the workshop, please contact <a href="mailto:slowdnn.workshop@gmail.com">slowdnn.workshop@gmail.com</a></footer></div><div class="main" id="top"><div id="main-header" class="main-header"><div></div><nav aria-label="Auxiliary" class="aux-nav"><ul class="aux-nav-list"><li class="aux-nav-list-item"> <a href="https://openreview.net/group?id=mbzuai.ac.ae/SLowDNN/2023/Workshop" class="site-button" > Submissions </a><li class="aux-nav-list-item"> <a href="https://docs.google.com/forms/d/e/1FAIpQLScMqcyVsrldFpZPwjajr2hcYz9aKx5V3riFNAEUQ7vswlrw7g/viewform" class="site-button" > Register </a></ul></nav></div><div id="main-content-wrap" class="main-content-wrap"><div id="main-content" class="main-content" role="main"><div class="splash"> <img src="/assets/images/splash0.jpg" alt="Splash photo of MBZUAI" /><div class="topleft"> Third Workshop on Seeking Low&#8209;Dimensionality in Deep&nbsp;Neural&nbsp;Networks</div><div class="bottomright"> January 2023,&nbsp;<a href="https://mbzuai.ac.ae/" target="_blank">MBZUAI</a></div></div><h1 id="tutorials"> <a href="#tutorials" class="anchor-heading" aria-labelledby="tutorials"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Tutorials</h1><p>The first day of the workshop features tutorial presentations from a subset of the organizers. These tutorials present an up-to-date account of the intersection between low-dimensional modeling and deep learning in an accessible format.</p><p>The tutorials are summarized below. See <a href="/schedule">the schedule</a> for the precise times of each tutorial. Some of the tutorials draw on material from <a href="https://highdimdata-lowdimmodels-tutorial.github.io/">an ICASSP 2022 short course</a>.</p><h3 id="introduction-to-low-dimensional-models"> <a href="#introduction-to-low-dimensional-models" class="anchor-heading" aria-labelledby="introduction-to-low-dimensional-models"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Introduction to Low-Dimensional Models</h3><div class="organizer" style="width: 310px;"> <img class="organizer-image" src="/assets/images/jw.jpg" alt="" /><div style="padding: 20px 0;"><h3 class="organizer-name"> <a href="#introduction-to-low-dimensional-models" class="anchor-heading" aria-labelledby="introduction-to-low-dimensional-models"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="http://www.columbia.edu/~jw2966/">John Wright</a></h3><p>Columbia University</p></div></div><p>The first session will introduce fundamental properties and theoretical results for sensing, processing, analyzing, and learning low-dimensional structures from high-dimensional data. We will first discuss classical low-dimensional models, such as sparse recovery and low-rank matrix sensing, and motivate these models by applications in medical imaging, collaborative filtering, face recognition, and beyond. Based on convex relaxation, we will characterize the conditions, in terms of sample/data complexity, under which the inverse problems of recovering such low-dimensional structures become tractable and can be solved efficiently, with guaranteed correctness or accuracy.</p><h3 id="nonconvex-optimization-of-low-dimensional-models"> <a href="#nonconvex-optimization-of-low-dimensional-models" class="anchor-heading" aria-labelledby="nonconvex-optimization-of-low-dimensional-models"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Nonconvex Optimization of Low-Dimensional Models</h3><div class="organizer" style="width: 310px;"> <img class="organizer-image" src="/assets/images/yz.jpg" alt="" /><div style="padding: 20px 0;"><h3 class="organizer-name"> <a href="#nonconvex-optimization-of-low-dimensional-models" class="anchor-heading" aria-labelledby="nonconvex-optimization-of-low-dimensional-models"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://sites.google.com/view/yuqianzhang">Yuqian Zhang</a></h3><p>Rutgers University</p></div></div><p>We will transit from sensing to learning low-dimensional structures, such as dictionary learning, sparse blind deconvolution, and dual principal component analysis. Problems associated with learning low-dimensional models from sample data are often nonconvex: either they do not have tractable convex relaxations or the nonconvex formulation is preferred due to physical or computational constraints (such as limited memory). To deal with these challenges, we will introduce a systematic approach of analyzing the corresponding nonconvex landscapes from a geometry and symmetry perspective. The resulting approach leads to provable globally convergent nonconvex optimization methods.</p><h3 id="learning-low-dimensional-structure-via-deep-networks"> <a href="#learning-low-dimensional-structure-via-deep-networks" class="anchor-heading" aria-labelledby="learning-low-dimensional-structure-via-deep-networks"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Learning Low-Dimensional Structure via Deep Networks</h3><div class="organizer" style="width: 310px;"> <img class="organizer-image" src="/assets/images/sdb.jpg" alt="" /><div style="padding: 20px 0;"><h3 class="organizer-name"> <a href="#learning-low-dimensional-structure-via-deep-networks" class="anchor-heading" aria-labelledby="learning-low-dimensional-structure-via-deep-networks"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://sdbuchanan.com/">Sam Buchanan</a></h3><p>TTIC</p></div></div><p>We will discuss the contemporary topic of using deep models for computing with nonlinear data, introducing strong conceptual connections between low-dimensional structures in data and deep models. We will then consider a mathematical model problem that attempts to capture these aspects of practice, and show how low-dimensional structure in data and tasks influences the resources (statistical, architectural) required to achieve a given performance level. Our discussion will revolve around basic tradeoffs between these resources and theoretical guarantees of performance.</p><h3 id="learned-representations-and-low-dimensional-structures"> <a href="#learned-representations-and-low-dimensional-structures" class="anchor-heading" aria-labelledby="learned-representations-and-low-dimensional-structures"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Learned Representations and Low-Dimensional Structures</h3><div class="organizer" style="width: 310px;"> <img class="organizer-image" src="/assets/images/zz.jpeg" alt="" /><div style="padding: 20px 0;"><h3 class="organizer-name"> <a href="#learned-representations-and-low-dimensional-structures" class="anchor-heading" aria-labelledby="learned-representations-and-low-dimensional-structures"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://zhihuizhu.github.io">Zhihui Zhu</a></h3><p>Ohio State University</p></div></div><p>Continuing our exploration of deep models for nonlinear data, we will begin to delve into learned representations, network architectures, regularizations, and beyond. We will see how the tools for nonconvexity developed previously shed light on the learned representations produced by deep networks, through connections to matrix factorization. We will observe how algorithms that interact with data will expose additional connections to low-dimensional models, through implicit regularization of the network parameters.</p><h3 id="design-deep-networks-for-pursuing-low-dimensional-structures"> <a href="#design-deep-networks-for-pursuing-low-dimensional-structures" class="anchor-heading" aria-labelledby="design-deep-networks-for-pursuing-low-dimensional-structures"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Design Deep Networks for Pursuing Low-Dimensional Structures</h3><div class="organizer" style="width: 310px;"> <img class="organizer-image" src="/assets/images/ma.jpeg" alt="" /><div style="padding: 20px 0;"><h3 class="organizer-name"> <a href="#design-deep-networks-for-pursuing-low-dimensional-structures" class="anchor-heading" aria-labelledby="design-deep-networks-for-pursuing-low-dimensional-structures"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://people.eecs.berkeley.edu/~yima/">Yi Ma</a></h3><p>UC Berkeley</p></div></div><p>Based upon the previous discussion on the connection between low-dimensional structures and deep models, in this section, we will discuss principles for designing deep networks through the lens of learning good low-dimensional representation for (potentially nonlinear) low-dimensional structures. We will see how unrolling iterative optimization algorithms for low-dimensional problems (such as the sparsifying algorithms) naturally lead to deep neural networks. We will then show how modern deep layered architectures, linear (convolution) operators, and nonlinear activations, and even all parameters can be derived from the principle of learning a compact linear discriminative representation for nonlinear low-dimensional structures within the data. We will show how so learned representations can bring tremendous benefits in tasks such as learning generative models, noise stability, and incremental learning.</p><h3 id="sparsity-neural-networks--practice-and-theory"> <a href="#sparsity-neural-networks--practice-and-theory" class="anchor-heading" aria-labelledby="sparsity-neural-networks--practice-and-theory"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Sparsity Neural Networks – Practice and Theory</h3><div class="organizer" style="width: 310px;"> <img class="organizer-image" src="/assets/images/aw.png" alt="" /><div style="padding: 20px 0;"><h3 class="organizer-name"> <a href="#sparsity-neural-networks--practice-and-theory" class="anchor-heading" aria-labelledby="sparsity-neural-networks--practice-and-theory"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://www.ece.utexas.edu/people/faculty/atlas-wang">Atlas Wang</a></h3><p>UT Austin</p></div></div><p>We discuss the role of sparsity in general neural network architectures, and shed light on how sparsity interacts with deep learning under the overparameterization regime, for both practitioners and theorists. A sparse neural network (NN) has most of its parameters set to zero and is traditionally considered as the product of NN compression (i.e., pruning). Yet recently, sparsity has exposed itself as an important bridge for modeling the underlying low dimensionality of NNs, for understanding their generalization, optimization dynamics, implicit regularization, expressivity, and robustness. Deep NNs learned with sparsity-aware priors have also demonstrated significantly improved performances through a full stack of applied work on algorithms, systems, and hardware. In this talk, I plan to cover recent progress on the practical, theoretical, and scientific aspects of sparse NNs. I will try scratching the surface of three aspects – (1) practically, why one should love a sparse NN, beyond just a post-training NN compression tool; (2) theoretically, what are some guarantees that one can expect from sparse NNs; and (3) what is future prospect of exploiting sparsity in NNs.</p><h3 id="advancing-machine-learning-for-imaging--regularization-and-robustness"> <a href="#advancing-machine-learning-for-imaging--regularization-and-robustness" class="anchor-heading" aria-labelledby="advancing-machine-learning-for-imaging--regularization-and-robustness"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Advancing Machine Learning for Imaging – Regularization and Robustness</h3><div class="organizer" style="width: 310px;"> <img class="organizer-image" src="/assets/images/sr.png" alt="" /><div style="padding: 20px 0;"><h3 class="organizer-name"> <a href="#advancing-machine-learning-for-imaging--regularization-and-robustness" class="anchor-heading" aria-labelledby="advancing-machine-learning-for-imaging--regularization-and-robustness"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://sites.google.com/site/sairavishankar3/">Saiprasad Ravishankar</a></h3><p>Michigan State University</p></div></div><p>In this talk, we present our work on improving machine learning for image reconstruction on three fronts – i) learning regularizers, ii) learning with no training data, and iii) ensuring robustness to perturbations in learning-based schemes. First, we present an approach for supervised learning of sparsity-promoting regularizers, where the parameters of the regularizer are learned to minimize reconstruction error on a paired training set. Training involves a challenging bilevel optimization problem with a nonsmooth lower-level objective. We derive an expression for the gradient of the training loss using the implicit closed-form solution of the lower-level variational problem, and provide an accompanying exact gradient descent algorithm (dubbed BLORC). Our experiments show that the gradient computation is efficient and BLORC learns meaningful operators for effective denoising. Second, we investigate the deep image prior (DIP) scheme that recovers an image by fitting an overparameterized neural network directly to the image’s corrupted measurements. To address DIP’s overfitting and performance issues, recent work proposed using a reference image as the network input. However, obtaining the reference often requires supervision. Hence, we propose a self-guided scheme that uses only undersampled measurements to estimate both the network weights and input image. We exploit regularization requiring the network to be a powerful denoiser. Our self-guided method gives significantly improved reconstructions for MRI with limited measurements compared to recent schemes, while using no training data. Finally, recent studies have shown that trained deep reconstruction models could be over-sensitive to tiny input perturbations, which cause unstable, low-quality reconstructed images. To address this issue, we propose Smoothed Unrolling (SMUG), which incorporates a randomized smoothing-based robust learning operation into a deep unrolling architecture and improves the robustness of MRI reconstruction with respect to diverse perturbations.</p></div></div></div>
