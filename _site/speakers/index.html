<!DOCTYPE html><html lang="en-US"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=Edge"><link rel="shortcut icon" href="/favicon.ico" type="image/x-icon"><link rel="stylesheet" href="/assets/css/just-the-docs-default.css"> <script src="/assets/js/just-the-docs.js"></script><meta name="viewport" content="width=device-width, initial-scale=1"><title>Speakers | SLowDNN Workshop</title><meta name="generator" content="Jekyll v3.9.2" /><meta property="og:title" content="Speakers" /><meta property="og:locale" content="en_US" /><meta name="description" content="A listing of confirmed speakers that will present at the workshop." /><meta property="og:description" content="A listing of confirmed speakers that will present at the workshop." /><link rel="canonical" href="http://localhost:4000/speakers/" /><meta property="og:url" content="http://localhost:4000/speakers/" /><meta property="og:site_name" content="SLowDNN Workshop" /><meta property="og:type" content="website" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Speakers" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"WebPage","description":"A listing of confirmed speakers that will present at the workshop.","headline":"Speakers","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"http://localhost:4000/assets/images/logo1.png"}},"url":"http://localhost:4000/speakers/"}</script><body> <a class="skip-to-main" href="#main-content">Skip to main content</a> <svg xmlns="http://www.w3.org/2000/svg" style="display: none;"> <symbol id="svg-link" viewBox="0 0 24 24"><title>Link</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"><title>Search</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"><title>Menu</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"><title>Expand</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"><polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"><title>Document</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"><path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> <symbol id="svg-external-link" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link"><title id="svg-external-link-title">(external link)</title><path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line> </symbol> </svg><div class="side-bar"><div class="site-header"> <a href="/" class="site-title lh-tight"><div class="site-logo"></div></a> <a href="#" id="menu-button" class="site-button"> <svg viewBox="0 0 24 24" class="icon"><use xlink:href="#svg-menu"></use></svg> </a></div><nav aria-label="Main" id="site-nav" class="site-nav"><ul class="nav-list"><li class="nav-list-item"><a href="/index.html" class="nav-list-link">Home</a><li class="nav-list-item"><a href="/submission/" class="nav-list-link">Call for Papers</a><li class="nav-list-item"><a href="/schedule/" class="nav-list-link">Schedule</a><li class="nav-list-item active"><a href="/speakers/" class="nav-list-link active">Speakers</a><li class="nav-list-item"><a href="/travel/" class="nav-list-link">Travel</a><li class="nav-list-item"><a href="/organizers/" class="nav-list-link">Organizers</a><li class="nav-list-item"><a href="/sponsors/" class="nav-list-link">Host Institution</a><li class="nav-list-item"><a href="/past_workshops/" class="nav-list-link">Past workshops</a></ul></nav><footer class="site-footer"> For inquiries about the workshop, please contact <a href="mailto:slowdnn.workshop@gmail.com">slowdnn.workshop@gmail.com</a></footer></div><div class="main" id="top"><div id="main-header" class="main-header"><div></div><nav aria-label="Auxiliary" class="aux-nav"><ul class="aux-nav-list"><li class="aux-nav-list-item"> <a href="https://openreview.net/group?id=mbzuai.ac.ae/SLowDNN/2023/Workshop" class="site-button" > Submissions </a><li class="aux-nav-list-item"> <a href="https://docs.google.com/forms/d/e/1FAIpQLScMqcyVsrldFpZPwjajr2hcYz9aKx5V3riFNAEUQ7vswlrw7g/viewform" class="site-button" > Register </a></ul></nav></div><div id="main-content-wrap" class="main-content-wrap"><div id="main-content" class="main-content" role="main"><div class="splash"> <img src="/assets/images/splash0.jpg" alt="Splash photo of MBZUAI" /><div class="topleft"> Third Workshop on Seeking Low&#8209;Dimensionality in Deep&nbsp;Neural&nbsp;Networks</div><div class="bottomright"> January 2023,&nbsp;<a href="https://mbzuai.ac.ae/" target="_blank">MBZUAI</a></div></div><h1 id="confirmed-speakers"> <a href="#confirmed-speakers" class="anchor-heading" aria-labelledby="confirmed-speakers"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Confirmed Speakers</h1><div style="clear: both; display: flex; flex-wrap: wrap; justify-content: flex-start;"><div class="speaker" style="width: 150px;"> <img class="speaker-image" src="/assets/images/speakers/belkin.jpeg" alt="" /><div><h3 class="speaker-name"> <a href="http://misha.belkin-wang.org/">Misha Belkin</a></h3><p>UC San Diego</p></div></div><div class="speaker" style="width: 150px;"> <img class="speaker-image" src="/assets/images/speakers/bronstein.jpeg" alt="" /><div><h3 class="speaker-name"> <a href="https://www.imperial.ac.uk/people/m.bronstein">Michael Bronstein</a></h3><p>University of Oxford</p></div></div><div class="speaker" style="width: 150px;"> <img class="speaker-image" src="/assets/images/speakers/chen0.jpeg" alt="" /><div><h3 class="speaker-name"> <a href="https://cs.stanford.edu/people/beidic/">Beidi Chen</a></h3><p>Meta</p></div></div><div class="speaker" style="width: 150px;"> <img class="speaker-image" src="/assets/images/speakers/dokmanic.jpeg" alt="" /><div><h3 class="speaker-name"> <a href="https://dmi.unibas.ch/de/personen/ivan-dokmanic/">Ivan Dokmanić</a></h3><p>University of Basel</p></div></div><div class="speaker" style="width: 150px;"> <img class="speaker-image" src="/assets/images/speakers/du0.jpeg" alt="" /><div><h3 class="speaker-name"> <a href="https://simonshaoleidu.com/">Simon Du</a></h3><p>University of Washington</p></div></div><div class="speaker" style="width: 150px;"> <img class="speaker-image" src="/assets/images/speakers/elghaoui.png" alt="" /><div><h3 class="speaker-name"> <a href="https://people.eecs.berkeley.edu/~elghaoui/">Laurent El Ghaoui</a></h3><p>UC Berkeley</p></div></div><div class="speaker" style="width: 150px;"> <img class="speaker-image" src="/assets/images/speakers/elad.jpeg" alt="" /><div><h3 class="speaker-name"> <a href="https://elad.cs.technion.ac.il/">Michael Elad</a></h3><p>Technion</p></div></div><div class="speaker" style="width: 150px;"> <img class="speaker-image" src="/assets/images/speakers/heckel.jpeg" alt="" /><div><h3 class="speaker-name"> <a href="https://reinhardheckel.com/">Reinhard Heckel</a></h3><p>TU Munich</p></div></div><div class="speaker" style="width: 150px;"> <img class="speaker-image" src="/assets/images/speakers/kutyniok.jpg" alt="" /><div><h3 class="speaker-name"> <a href="https://www.ai.math.uni-muenchen.de/members/professor/kutyniok/index.html">Gitta Kutyniok</a></h3><p>LMU Munich</p></div></div><div class="speaker" style="width: 150px;"> <img class="speaker-image" src="/assets/images/speakers/lei0.jpeg" alt="" /><div><h3 class="speaker-name"> <a href="https://cecilialeiqi.github.io/">Qi Lei</a></h3><p>NYU</p></div></div><div class="speaker" style="width: 150px;"> <img class="speaker-image" src="/assets/images/speakers/panigrahy.jpeg" alt="" /><div><h3 class="speaker-name"> <a href="http://theory.stanford.edu/~rinap/">Rina Panigrahy</a></h3><p>Google</p></div></div><div class="speaker" style="width: 150px;"> <img class="speaker-image" src="/assets/images/speakers/poggio0.jpeg" alt="" /><div><h3 class="speaker-name"> <a href="https://mcgovern.mit.edu/profile/tomaso-poggio/">Tomaso Poggio</a></h3><p>MIT</p></div></div><div class="speaker" style="width: 150px;"> <img class="speaker-image" src="/assets/images/speakers/shamir.jpeg" alt="" /><div><h3 class="speaker-name"> <a href="https://www.wisdom.weizmann.ac.il/~shamiro/">Ohad Shamir</a></h3><p>Weizmann Institute</p></div></div><div class="speaker" style="width: 150px;"> <img class="speaker-image" src="/assets/images/speakers/soltanolkotabi.jpeg" alt="" /><div><h3 class="speaker-name"> <a href="https://viterbi-web.usc.edu/~soltanol/">Mahdi Soltanolkotabi</a></h3><p>USC</p></div></div><div class="speaker" style="width: 150px;"> <img class="speaker-image" src="/assets/images/speakers/soudry.png" alt="" /><div><h3 class="speaker-name"> <a href="https://sites.google.com/site/danielsoudry/">Daniel Soudry</a></h3><p>Technion</p></div></div><div class="speaker" style="width: 150px;"> <img class="speaker-image" src="/assets/images/speakers/su.jpeg" alt="" /><div><h3 class="speaker-name"> <a href="http://stat.wharton.upenn.edu/~suw/">Weijie Su</a></h3><p>UPenn</p></div></div><div class="speaker" style="width: 150px;"> <img class="speaker-image" src="/assets/images/speakers/vidal.jpeg" alt="" /><div><h3 class="speaker-name"> <a href="http://vision.jhu.edu/rvidal.html">René Vidal</a></h3><p>Johns Hopkins</p></div></div><div class="speaker" style="width: 150px;"> <img class="speaker-image" src="/assets/images/speakers/wang.jpeg" alt="" /><div><h3 class="speaker-name"> <a href="https://sites.cs.ucsb.edu/~yuxiangw/">Yu-Xiang Wang</a></h3><p>UCSB</p></div></div><div class="speaker" style="width: 150px;"> <img class="speaker-image" src="/assets/images/speakers/wen.jpeg" alt="" /><div><h3 class="speaker-name"> <a href="https://personal.ntu.edu.sg/bihan.wen/">Bihan Wen</a></h3><p>NTU</p></div></div><div class="speaker" style="width: 150px;"> <img class="speaker-image" src="/assets/images/speakers/yang.png" alt="" /><div><h3 class="speaker-name"> <a href="https://sml.inf.ethz.ch/group/fannyy/">Fanny Yang</a></h3><p>ETH Zurich</p></div></div></div><h1 id="talk-details"> <a href="#talk-details" class="anchor-heading" aria-labelledby="talk-details"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Talk Details</h1><h3 id="misha-belkin"> <a href="#misha-belkin" class="anchor-heading" aria-labelledby="misha-belkin"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="http://misha.belkin-wang.org/">Misha Belkin</a></h3><p>UC San Diego</p><h4 id="title-why-do-neural-models-need-so-many-parameters"> <a href="#title-why-do-neural-models-need-so-many-parameters" class="anchor-heading" aria-labelledby="title-why-do-neural-models-need-so-many-parameters"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Title: Why do neural models need so many parameters?</h4><h4 id="abstract"> <a href="#abstract" class="anchor-heading" aria-labelledby="abstract"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Abstract</h4><p>One of the striking aspects of modern neural networks is their extreme size reaching billions or even trillions parameters. Why are so many parameters needed? To attempt an answer to this question, I will discuss an algorithm and distribution independent non-asymptotic trade-off between the model size, excess test loss, and training loss for linear predictors and feature maps. Specifically, we show that models that perform well on the test data (have low excess loss) are either “classical” – have training loss close to the noise level, or are “modern” – have a much larger number of parameters compared to the minimum needed to fit the training data exactly. Furthermore, I will provide empirical evidence that optimal performance of realistic models is typically achieved in the “modern” regime, when they are trained below the noise level.</p><h3 id="michael-bronstein"> <a href="#michael-bronstein" class="anchor-heading" aria-labelledby="michael-bronstein"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://www.imperial.ac.uk/people/m.bronstein">Michael Bronstein</a></h3><p>University of Oxford</p><h4 id="title-tba"> <a href="#title-tba" class="anchor-heading" aria-labelledby="title-tba"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Title: TBA</h4><h4 id="abstract-1"> <a href="#abstract-1" class="anchor-heading" aria-labelledby="abstract-1"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Abstract</h4><p>TBA</p><h3 id="beidi-chen"> <a href="#beidi-chen" class="anchor-heading" aria-labelledby="beidi-chen"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://cs.stanford.edu/people/beidic/">Beidi Chen</a></h3><p>Meta</p><h4 id="title-hardware-aware-sparsity-accurate-and-efficient-foundation-model-training"> <a href="#title-hardware-aware-sparsity-accurate-and-efficient-foundation-model-training" class="anchor-heading" aria-labelledby="title-hardware-aware-sparsity-accurate-and-efficient-foundation-model-training"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Title: Hardware-aware Sparsity: Accurate and Efficient Foundation Model Training</h4><h4 id="abstract-2"> <a href="#abstract-2" class="anchor-heading" aria-labelledby="abstract-2"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Abstract</h4><p>Foundation models trained on complex and rapidly growing data consume enormous computational resources. In this talk, I will describe our recent work on exploiting model and activation sparsity to accelerate foundation model training in both data and model parallel settings. We show that adapting algorithms on current hardware leads to efficient model training with no drop in accuracy. I will start by describing Pixelated Butterfly and Monarch, simple yet efficient sparse model training frameworks on GPUs. They use simple static block-sparse patterns based on butterfly and low-rank matrices, taking into account GPU block-oriented efficiency. They train up to 2.5x faster (wall-clock) than the dense Vision Transformer and GPT-2 counterparts with no drop in accuracy. Next, I will present AC-SGD, communication-efficient pipeline parallelism training frameworks over slow networks. Based on an interesting observation that model weights change slowly during the training, AC-SGD compresses activations or the change of activations with guarantees. It trains or fine-tunes DeBERTa and GPT2-1.5B 4.3x faster in slower networks without sacrificing model quality. I will conclude by outlining three future research directions - data efficiency,software-hardware codesign, and ML for science, and several ongoing projects including linear-time algorithm for large optimal transport problems, efficient autoregressive model (gpt3-style) inference, and ML for new material discovery.</p><h3 id="ivan-dokmanić"> <a href="#ivan-dokmanić" class="anchor-heading" aria-labelledby="ivan-dokmanić"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://dmi.unibas.ch/de/personen/ivan-dokmanic/">Ivan Dokmanić</a></h3><p>University of Basel</p><h4 id="title-statistical-mechanics-of-graph-neural-networks"> <a href="#title-statistical-mechanics-of-graph-neural-networks" class="anchor-heading" aria-labelledby="title-statistical-mechanics-of-graph-neural-networks"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Title: Statistical Mechanics of Graph Neural Networks</h4><h4 id="abstract-3"> <a href="#abstract-3" class="anchor-heading" aria-labelledby="abstract-3"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Abstract</h4><p>Graph convolution networks are excellent models for relational data but their success is not well understood. I will show how ideas from statistical physics and random matrix theory allow us to precisely characterize GCN generalization on the contextual stochastic block model—a community-structured graph model with features. The resulting curves are rich: they predict double descent thus far unseen in graph learning and explain the qualitative distinction between learning on homophilic graphs (such as friendship networks) and heterophilic graphs (such as protein interaction networks). Earlier approaches based on VC-dimension or Rademacher complexity are too blunt to yield similar mechanistic insight. Our findings pleasingly translate to real “production-scale” networks and datasets and suggest simple redesigns which improve performance of state-of-the-art networks on heterophilic datasets. They further suggest intriguing connections with spectral graph theory, signal processing, and iterative methods for the Helmholtz equation. Joint work with Cheng Shi, Liming Pan, and Hong Hu.</p><h3 id="simon-du"> <a href="#simon-du" class="anchor-heading" aria-labelledby="simon-du"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://simonshaoleidu.com/">Simon Du</a></h3><p>University of Washington</p><h4 id="title-passive-and-active-multi-task-representation-learning"> <a href="#title-passive-and-active-multi-task-representation-learning" class="anchor-heading" aria-labelledby="title-passive-and-active-multi-task-representation-learning"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Title: Passive and Active Multi-Task Representation Learning</h4><h4 id="abstract-4"> <a href="#abstract-4" class="anchor-heading" aria-labelledby="abstract-4"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Abstract</h4><p>Representation learning has been widely used in many applications. In this talk, I will present our work which uncovers when and why representation learning provably improves the sample efficiency, from a statistical learning point of view. Furthermore, I will talk about how to actively select the most relevant task to boost the performance.</p><h3 id="laurent-el-ghaoui"> <a href="#laurent-el-ghaoui" class="anchor-heading" aria-labelledby="laurent-el-ghaoui"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://people.eecs.berkeley.edu/~elghaoui/">Laurent El Ghaoui</a></h3><p>UC Berkeley</p><h4 id="title-implicit-rules-in-deep-learning"> <a href="#title-implicit-rules-in-deep-learning" class="anchor-heading" aria-labelledby="title-implicit-rules-in-deep-learning"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Title: Implicit rules in deep learning</h4><h4 id="abstract-5"> <a href="#abstract-5" class="anchor-heading" aria-labelledby="abstract-5"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Abstract</h4><p>Recently, prediction rules based on so-called implicit models have emerged as a new, high-potential paradigm in deep learning. These models rely on an “equilibrium” equation to define the prediction, instead of a recurrence through multiple layers. Currently, even very complex deep learning models are based on a “feedforward” structure, without loops, and as such the popular term “neural” applied to such models is not fully warranted, since the brain itself possesses loops. Allowing for loops may be the key to describing complex higher-level reasoning, which has so far eluded the deep learning paradigms. However, it raises the fundamental issue of well-posedness, since there may be no or multiple solutions to the corresponding equilibrium equation. In this talk, I will review some aspects of implicit models, starting from a unifying “state-space” representation that greatly simplifies notation. I will also introduce various training problems of implicit models, including one that allows convex optimization, and their connections to topics such as architecture optimization, model compression, and robustness. In addition, the talk will show the potential of implicit models through experimental results on various problems such as parameter reduction, feature elimination, and mathematical reasoning tasks.</p><h3 id="michael-elad"> <a href="#michael-elad" class="anchor-heading" aria-labelledby="michael-elad"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://elad.cs.technion.ac.il/">Michael Elad</a></h3><p>Technion</p><h4 id="title-image-denoising---not-what-you-think"> <a href="#title-image-denoising---not-what-you-think" class="anchor-heading" aria-labelledby="title-image-denoising---not-what-you-think"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Title: Image Denoising - Not What You Think</h4><h4 id="abstract-6"> <a href="#abstract-6" class="anchor-heading" aria-labelledby="abstract-6"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Abstract</h4><p>Image denoising – removal of white additive Gaussian noise from an image – is one of the oldest and most studied problems in image processing. An extensive work over several decades has led to thousands of papers on this subject, and to many well-performing algorithms for this task. As expected, the era of deep learning has brought yet another revolution to this subfield, and took the lead in today’s ability for noise suppression in images. All this progress has led some researchers to believe that “denoising is dead”, in the sense that all that can be achieved is already done. Exciting as all this story might be, this talk IS NOT ABOUT IT! Our story focuses on recently discovered abilities and vulnerabilities of image denoisers. In a nut-shell, we expose the possibility of using image denoisers for serving other problems, such as regularizing general inverse problems and serving as the engine for image synthesis. We also unveil the (strange?) idea that denoising (and other inverse problems) might not have a unique solution, as common algorithms would have you believe. Instead, we will describe constructive ways to produce randomized and diverse high perceptual quality results for inverse problems.</p><h3 id="reinhard-heckel"> <a href="#reinhard-heckel" class="anchor-heading" aria-labelledby="reinhard-heckel"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://reinhardheckel.com/">Reinhard Heckel</a></h3><p>TU Munich</p><h4 id="title-the-role-of-data-and-models-for-deep-learning-based-image-reconstruction"> <a href="#title-the-role-of-data-and-models-for-deep-learning-based-image-reconstruction" class="anchor-heading" aria-labelledby="title-the-role-of-data-and-models-for-deep-learning-based-image-reconstruction"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Title: The role of data and models for deep-learning based image reconstruction</h4><h4 id="abstract-7"> <a href="#abstract-7" class="anchor-heading" aria-labelledby="abstract-7"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Abstract</h4><p>Deep-learning methods give state-of-the-art performance for a variety of imaging tasks, including accelerated magnetic resonance imaging. In this talk we discuss whether improved models and algorithms or training data are the most promising way forward. First, we ask whether increasing the model size and the training data improves performance in a similar fashion as it has in domains such as language modeling. We find that scaling beyond relatively few examples yields only marginal performance gains. Second, we discuss the robustness of deep learning based image reconstruction methods. Perhaps surprisingly, we find no evidence for neural networks being any less robust than classical reconstruction methods (such as l1 minimization). However, we find that both classical and deep learning based approaches perform significantly worse under distribution shifts, i.e., when trained (or tuned) and tested on slightly different data. Finally, we show that the out-of-distribution performance can be improved through more diverse training data, or through an algorithmic intervention called test-time-training.</p><h3 id="gitta-kutyniok"> <a href="#gitta-kutyniok" class="anchor-heading" aria-labelledby="gitta-kutyniok"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://www.ai.math.uni-muenchen.de/members/professor/kutyniok/index.html">Gitta Kutyniok</a></h3><p>LMU Munich</p><h4 id="title-the-next-generation-of-reliable-ai-from-digital-to-analog-hardware"> <a href="#title-the-next-generation-of-reliable-ai-from-digital-to-analog-hardware" class="anchor-heading" aria-labelledby="title-the-next-generation-of-reliable-ai-from-digital-to-analog-hardware"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Title: The Next Generation of Reliable AI: From Digital to Analog Hardware</h4><h4 id="abstract-8"> <a href="#abstract-8" class="anchor-heading" aria-labelledby="abstract-8"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Abstract</h4><p>Artificial intelligence is currently leading to one breakthrough after the other, in the sciences, in industry, and in public life. However, one current major drawback is the lack of reliability of such methodologies, which, in particular, also concerns almost any application of deep neural networks. In this lecture, we will first provide a short introduction into the world of reliability of deep neural networks, with one main focus being on explainability. We will, in particular, present a novel approach based on information theory, coined ShearletX, which allows to not only provide higher level explanations, but also reveals the reason for wrong decisions. We will then delve deeper and discuss fundamental limitations of numerous current deep learning-based approaches, showing that there do exist severe problems in terms of computability on any type of digital hardware, which seriously affects their reliability. But theory also shows a way out, pointing towards a future on analog hardware such as neuromorphic computing or quantum computing to achieve true reliability.</p><h3 id="qi-lei"> <a href="#qi-lei" class="anchor-heading" aria-labelledby="qi-lei"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://cecilialeiqi.github.io/">Qi Lei</a></h3><p>NYU</p><h4 id="title-reconstructing-training-data-from-model-gradient-provably"> <a href="#title-reconstructing-training-data-from-model-gradient-provably" class="anchor-heading" aria-labelledby="title-reconstructing-training-data-from-model-gradient-provably"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Title: Reconstructing Training Data from Model Gradient, Provably</h4><h4 id="abstract-9"> <a href="#abstract-9" class="anchor-heading" aria-labelledby="abstract-9"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Abstract</h4><p>Understanding when and how much a model gradient leaks information about the training sample is an important question in privacy. In this talk, we present a surprising result: even without training or memorizing the data, we can fully reconstruct the training samples from a single gradient query at a randomly chosen parameter value. We prove the identifiability of the training data under mild conditions: with shallow or deep neural networks and a wide range of activation functions. We also present a statistically and computationally efficient algorithm based on low-rank tensor decomposition to reconstruct the training data. As a provable attack that reveals sensitive training data, our findings suggest potential severe threats to privacy, especially in federated learning.</p><h3 id="rina-panigrahy"> <a href="#rina-panigrahy" class="anchor-heading" aria-labelledby="rina-panigrahy"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="http://theory.stanford.edu/~rinap/">Rina Panigrahy</a></h3><p>Google</p><h4 id="title-how-to-learn-a-table-of-concepts"> <a href="#title-how-to-learn-a-table-of-concepts" class="anchor-heading" aria-labelledby="title-how-to-learn-a-table-of-concepts"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Title: How to learn a Table of Concepts?</h4><h4 id="abstract-10"> <a href="#abstract-10" class="anchor-heading" aria-labelledby="abstract-10"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Abstract</h4><p>An idealized view of an intelligent system is one where there is a table of concepts from simple to complex and on each raw input (image or text) is processed to identify the concepts present or triggered by that input. While today’s ML systems may use a Mixture of Experts or Table of entities/mentions that are trying to map concepts to experts/entities/mention, there is hardly a clear picture of how concepts build upon each other automatically. In the domain of images, we can think of commonly occurring shapes, types of motions as concepts and ask how would a visual system create entries for different shape types in a table of concepts. How does one convert raw data into concepts given by something like their latent representation. A trivial transformation like random projection doesn’t work. But we will see how simple networks similar to ones used in practice can be viewed as “sketching operators” that can be used to create representations for images with simple shapes that are isomorphic to their polynomial representations. By putting such polynomial representations in a locality sensitive table, we obtain a dictionary of curves. And then by recursively applying another layer of the sketching operator on the curves one gets a dictionary of concepts such as shapes.</p><h3 id="tomaso-poggio"> <a href="#tomaso-poggio" class="anchor-heading" aria-labelledby="tomaso-poggio"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://mcgovern.mit.edu/profile/tomaso-poggio/">Tomaso Poggio</a></h3><p>MIT</p><h4 id="title-a-key-principle-underlying-deep-networks-compositional-sparsity"> <a href="#title-a-key-principle-underlying-deep-networks-compositional-sparsity" class="anchor-heading" aria-labelledby="title-a-key-principle-underlying-deep-networks-compositional-sparsity"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Title: A key principle underlying deep networks: compositional sparsity</h4><h4 id="abstract-11"> <a href="#abstract-11" class="anchor-heading" aria-labelledby="abstract-11"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Abstract</h4><p>A key question is whether there exist a theoretical explanation — a common motif — to the various network architectures, including the human brain, that perform so well in learning tasks. I will discuss the conjecture that this is compositional sparsity of effectively computable functions: all functions of many variables must effectively be compositionally sparse that is with constituent functions each depending on a small number of variables.</p><h3 id="ohad-shamir"> <a href="#ohad-shamir" class="anchor-heading" aria-labelledby="ohad-shamir"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://www.wisdom.weizmann.ac.il/~shamiro/">Ohad Shamir</a></h3><p>Weizmann Institute</p><h4 id="title-implicit-bias-in-machine-learning"> <a href="#title-implicit-bias-in-machine-learning" class="anchor-heading" aria-labelledby="title-implicit-bias-in-machine-learning"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Title: Implicit bias in machine learning</h4><h4 id="abstract-12"> <a href="#abstract-12" class="anchor-heading" aria-labelledby="abstract-12"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Abstract</h4><p>Most practical algorithms for supervised machine learning boil down to optimizing the average performance over a training dataset. However, it is increasingly recognized that although the optimization objective is the same, the manner in which it is optimized plays a decisive role in the properties of the resulting predictor. For example, when training large neural networks, there are generally many weight combinations that will perfectly fit the training data. However, gradient-based training methods somehow tend to reach those which, for example, do not overfit; are brittle to adversarially crafted examples; or have other unusual properties. In this talk, I’ll describe several recent theoretical and empirical results related to this question.</p><h3 id="mahdi-soltanolkotabi"> <a href="#mahdi-soltanolkotabi" class="anchor-heading" aria-labelledby="mahdi-soltanolkotabi"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://viterbi-web.usc.edu/~soltanol/">Mahdi Soltanolkotabi</a></h3><p>USC</p><h4 id="title-demystifying-feature-learning-via-gradient-descent-with-applications-to-medical-image-reconstruction"> <a href="#title-demystifying-feature-learning-via-gradient-descent-with-applications-to-medical-image-reconstruction" class="anchor-heading" aria-labelledby="title-demystifying-feature-learning-via-gradient-descent-with-applications-to-medical-image-reconstruction"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Title: Demystifying Feature learning via gradient descent with applications to medical image reconstruction</h4><h4 id="abstract-13"> <a href="#abstract-13" class="anchor-heading" aria-labelledby="abstract-13"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Abstract</h4><p>In this talk I will discuss the challenges and opportunities for using deep learning in medical image reconstruction. Contemporary techniques in this field rely on convolutional architectures that are limited by the spatial invariance of their filters and have difficulty modeling long-range dependencies. To remedy this, I will discuss our work on designing new transformer-based architectures called HUMUS-Net that lead to state of the art performance and do not suffer from these limitations. A key component in the success of the above approach is a unique feature learning capability of unrolled neural networks trained based on end-to-end training. In the second part of the talk I will demystify this feature learning capability of neural networks in this context as well as more broadly for other problems. Our result is based on an intriguing spectral bias phenomena for gradient descent, that puts the iterations on a particular trajectory towards solutions that learn good features that generalize well. Notably this analysis overcomes a major theoretical bottleneck in the existing literature and goes beyond the “lazy” training regime which requires unrealistic hyperparameter choices (e.g. very small step sizes, large initialization or wide models).</p><h3 id="daniel-soudry"> <a href="#daniel-soudry" class="anchor-heading" aria-labelledby="daniel-soudry"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://sites.google.com/site/danielsoudry/">Daniel Soudry</a></h3><p>Technion</p><h4 id="title-how-catastrophic-can-catastrophic-forgetting-be-in-linear-regression"> <a href="#title-how-catastrophic-can-catastrophic-forgetting-be-in-linear-regression" class="anchor-heading" aria-labelledby="title-how-catastrophic-can-catastrophic-forgetting-be-in-linear-regression"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Title: How catastrophic can catastrophic forgetting be in linear regression?</h4><h4 id="abstract-14"> <a href="#abstract-14" class="anchor-heading" aria-labelledby="abstract-14"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Abstract</h4><p>Deep neural nets typically forget old tasks when trained on new tasks. This phenomenon, called “catastrophic forgetting” is not well understood, even in the most basic setting of linear regression. Therefore, we study catastrophic forgetting when fitting an overparameterized linear model to a sequence of tasks with different input distributions. We analyze how much the model forgets the true labels of earlier tasks after training on subsequent tasks, obtaining exact expressions and bounds. We establish connections between continual learning in the linear setting and two other research areas: alternating projections and the Kaczmarz method. In specific settings, we highlight differences between forgetting and convergence to the offline solution as studied in those areas. In particular, when T tasks in d dimensions are presented cyclically for k iterations, we prove an upper bound of T^2 * min{1/sqrt(k), d/k} on the forgetting. This stands in contrast to the convergence to the offline solution, which can be arbitrarily slow according to existing alternating projection results. We further show that the T^2 factor can be lifted when tasks are presented in a random ordering. Joint work with Itay Evron, Edward Moroshko, Rachel Ward, and Nati Srebro, published in COLT 22.</p><h3 id="weijie-su"> <a href="#weijie-su" class="anchor-heading" aria-labelledby="weijie-su"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="http://stat.wharton.upenn.edu/~suw/">Weijie Su</a></h3><p>UPenn</p><h4 id="title-geometrization-of-real-world-deep-neural-networks"> <a href="#title-geometrization-of-real-world-deep-neural-networks" class="anchor-heading" aria-labelledby="title-geometrization-of-real-world-deep-neural-networks"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Title: Geometrization of Real-World Deep Neural Networks</h4><h4 id="abstract-15"> <a href="#abstract-15" class="anchor-heading" aria-labelledby="abstract-15"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Abstract</h4><p>In this talk, we will investigate the emergence of geometric patterns in well-trained deep learning models by making use of a layer-peeled model and the law of equi-separation. The former is a nonconvex optimization program that models the last-layer features and weights. We use the model to shed light on the neural collapse phenomenon of Papyan, Han, and Donoho, and to predict a hitherto-unknown phenomenon that we term minority collapse in imbalanced training. This is based on joint work with Cong Fang, Hangfeng He, and Qi Long (arXiv:2101.12699). The law of equi-separation is a pervasive empirical phenomenon that describes how data are separated according to their class membership from the bottom to the top layer in a well-trained neural network. We will show that, through extensive computational experiments, neural networks improve data separation through layers in a simple exponential manner. This law leads to roughly equal ratios of separation that a single layer is able to improve, thereby showing that all layers are created equal. We will conclude the talk by discussing the implications of this law on the interpretation, robustness, and generalization of deep learning, as well as on the inadequacy of some existing approaches toward demystifying deep learning. This is based on joint work with Hangfeng He (arXiv:2210.17020).</p><h3 id="rené-vidal"> <a href="#rené-vidal" class="anchor-heading" aria-labelledby="rené-vidal"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="http://vision.jhu.edu/rvidal.html">René Vidal</a></h3><p>Johns Hopkins</p><h4 id="title-tba-1"> <a href="#title-tba-1" class="anchor-heading" aria-labelledby="title-tba-1"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Title: TBA</h4><h4 id="abstract-16"> <a href="#abstract-16" class="anchor-heading" aria-labelledby="abstract-16"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Abstract</h4><p>TBA</p><h3 id="yu-xiang-wang"> <a href="#yu-xiang-wang" class="anchor-heading" aria-labelledby="yu-xiang-wang"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://sites.cs.ucsb.edu/~yuxiangw/">Yu-Xiang Wang</a></h3><p>UCSB</p><h4 id="title-deep-learning-meets-nonparametric-regression-are-weight-decayed-dnns-locally-adaptive"> <a href="#title-deep-learning-meets-nonparametric-regression-are-weight-decayed-dnns-locally-adaptive" class="anchor-heading" aria-labelledby="title-deep-learning-meets-nonparametric-regression-are-weight-decayed-dnns-locally-adaptive"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Title: Deep Learning meets Nonparametric Regression: Are Weight-decayed DNNs locally adaptive?</h4><h4 id="abstract-17"> <a href="#abstract-17" class="anchor-heading" aria-labelledby="abstract-17"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Abstract</h4><p>They say deep learning is just curve fitting. But how good is it in curve fitting exactly? Are DNNs as good as, or even better than, classical curve-fitting tools, e.g., splines and wavelets? In this talk, I will cover my group’s recent paper on this topic and some interesting insight. Specifically, I will provide new answers to “Why is DNN stronger than kernels?” “Why are deep NNs stronger than shallow ones?”, “Why ReLU?”, “How do DNNs generalize under overparameterization?”, “What is the role of sparsity in deep learning?”, “Is lottery ticket hypothesis real?” All in one package. Intrigued? Come to my talk and find out!</p><h3 id="bihan-wen"> <a href="#bihan-wen" class="anchor-heading" aria-labelledby="bihan-wen"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://personal.ntu.edu.sg/bihan.wen/">Bihan Wen</a></h3><p>NTU</p><h4 id="title-beyond-classic-image-priors-deep-reinforcement-learning-and-disentangling-for-image-restoration"> <a href="#title-beyond-classic-image-priors-deep-reinforcement-learning-and-disentangling-for-image-restoration" class="anchor-heading" aria-labelledby="title-beyond-classic-image-priors-deep-reinforcement-learning-and-disentangling-for-image-restoration"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Title: Beyond Classic Image Priors: Deep Reinforcement Learning and Disentangling for Image Restoration</h4><h4 id="abstract-18"> <a href="#abstract-18" class="anchor-heading" aria-labelledby="abstract-18"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Abstract</h4><p>The key to solving various ill-posed inverse problems, including many computational imaging and image restoration tasks, is to exploit effective image priors. The classic signal processing theory laid the foundation on constructing analytical image models such as sparse coding and low-rank approximation. Recent advances in machine learning, especially deep learning technologies have made incredible progress in the past few years, which enables people to rethink how the image prior can be formulated more effective. Despite those many deep learning methods achieved the state-of-the-art results in image restoration tasks, there are still limitations in practice, such as adversarial robustness, customization, generalization, and data-efficiency. In this talk, I will share some of our recent works on deep disentangling and deep reinforcement learning in image restoration tasks. We argue that learning more than just the classic image priors are needed for solving inverse problems to alleviate some of the limitations. We show promising results in several image restoration tasks, including denoising, adversarial purification, low-light image enhancement and computational imaging.</p><h3 id="fanny-yang"> <a href="#fanny-yang" class="anchor-heading" aria-labelledby="fanny-yang"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://sml.inf.ethz.ch/group/fannyy/">Fanny Yang</a></h3><p>ETH Zurich</p><h4 id="title-strong-inductive-biases-provably-prevent-harmless-interpolation"> <a href="#title-strong-inductive-biases-provably-prevent-harmless-interpolation" class="anchor-heading" aria-labelledby="title-strong-inductive-biases-provably-prevent-harmless-interpolation"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Title: Strong inductive biases provably prevent harmless interpolation</h4><h4 id="abstract-19"> <a href="#abstract-19" class="anchor-heading" aria-labelledby="abstract-19"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Abstract</h4><p>Interpolating models have recently gained popularity in the statistical learning community due to common practices in modern machine learning: complex models achieve good generalization performance despite interpolating high-dimensional training data. In this talk, we prove generalization bounds for high-dimensional linear models that interpolate noisy data generated by a sparse ground truth. In particular, we first show that minimum-l1-norm interpolators achieve high-dimensional asymptotic consistency at a logarithmic rate. Further, as opposed to the regularized or noiseless case, for min-lp-norm interpolators with 1&lt;p&lt;2 we surprisingly obtain polynomial rates. Our results suggest a new trade-off for interpolating models: a stronger inductive bias encourages a simpler structure better aligned with the ground truth at the cost of an increased variance. We finally discuss our latest results where we show that this phenomenon also holds for nonlinear models.</p></div></div></div>
