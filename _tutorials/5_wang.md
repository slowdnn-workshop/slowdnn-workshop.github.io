---
name: Atlas Wang
role: Program Chair
email: atlaswang@utexas.edu
website: https://www.ece.utexas.edu/people/faculty/atlas-wang
affiliation: UT Austin
tutorial: Sparsity Neural Networks -- Practice and Theory
abstract: "We discuss the role of sparsity in general neural network architectures, and shed light on how sparsity interacts with deep learning under the overparameterization regime, for both practitioners and theorists. A sparse neural network (NN) has most of its parameters set to zero and is traditionally considered as the product of NN compression (i.e., pruning). Yet recently, sparsity has exposed itself as an important bridge for modeling the underlying low dimensionality of NNs, for understanding their generalization, optimization dynamics, implicit regularization, expressivity, and robustness. Deep NNs learned with sparsity-aware priors have also demonstrated significantly improved performances through a full stack of applied work on algorithms, systems, and hardware. In this talk, I plan to cover recent progress on the practical,  theoretical, and scientific aspects of sparse NNs. I will try scratching the surface of three aspects -- (1) practically, why one should love a sparse NN, beyond just a post-training NN compression tool; (2) theoretically, what are some guarantees that one can expect from sparse NNs; and (3) what is future prospect of exploiting sparsity in NNs."
photo: aw.png
---
